{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Length Effect Analysis\n",
    "\n",
    "Analyzes whether context-anchoring intensifies with conversation length.\n",
    "\n",
    "**Length categories:**\n",
    "- Short: 2-3 turns\n",
    "- Medium: 4-5 turns  \n",
    "- Long: 6+ turns\n",
    "\n",
    "**Focus:** X→EN fidelity (where context-anchoring is most visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Change to project root\n",
    "os.chdir(Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "MODELS = {\n",
    "    'gpt-5': 'GPT-5',\n",
    "    'gemini-3-pro': 'Gemini 3 Pro',\n",
    "    'claude-opus-4.5': 'Claude Opus 4.5',\n",
    "    'deepseek-v3.1': 'DeepSeek-V3.1',\n",
    "    'command-r-plus': 'Command R+',\n",
    "}\n",
    "\n",
    "LANGS = ['de', 'zh', 'es', 'ar']\n",
    "\n",
    "def categorize_length(turns):\n",
    "    \"\"\"Categorize conversation length.\"\"\"\n",
    "    if turns <= 3:\n",
    "        return 'Short'\n",
    "    elif turns <= 5:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Long'\n",
    "\n",
    "print(f\"Models: {list(MODELS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Turn Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze turn distribution in baseline dataset\nfrom collections import Counter\n\nwith open('data/experiments/baseline_en.jsonl') as f:\n    data = [json.loads(l) for l in f]\n    turns = [len(d['CONVERSATION']) for d in data]\n    axes = [d['AXIS'] for d in data]\n\nprint(\"Turn Distribution in Dataset:\")\nprint(\"=\" * 40)\nfor t, count in sorted(Counter(turns).items()):\n    pct = count / len(turns) * 100\n    bar = '█' * int(pct / 2)\n    print(f\"  {t:2d} turns: {count:3d} ({pct:5.1f}%) {bar}\")\n\nprint(f\"\\nTotal samples: {len(turns)}\")\nprint(f\"\\nBy category:\")\nshort = sum(1 for t in turns if t <= 3)\nmedium = sum(1 for t in turns if 4 <= t <= 5)\nlong = sum(1 for t in turns if t >= 6)\nprint(f\"  Short (2-3 turns):  {short:3d} ({short/len(turns)*100:.1f}%)\")\nprint(f\"  Medium (4-5 turns): {medium:3d} ({medium/len(turns)*100:.1f}%)\")\nprint(f\"  Long (6+ turns):    {long:3d} ({long/len(turns)*100:.1f}%)\")\n\nprint(f\"\\nBy axis:\")\nfor axis, count in Counter(axes).items():\n    print(f\"  {axis}: {count}\")\n\n# Turn distribution by axis\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"Turn Distribution BY AXIS:\")\nprint(\"=\" * 60)\n\nfor axis in ['INFERENCE_MEMORY', 'INSTRUCTION_RETENTION']:\n    axis_data = [d for d in data if d['AXIS'] == axis]\n    axis_turns = [len(d['CONVERSATION']) for d in axis_data]\n    \n    print(f\"\\n{axis} (n={len(axis_data)}):\")\n    print(f\"  {'Turns':<10} {'Count':>8} {'Pct':>8}\")\n    print(\"-\" * 30)\n    for t, count in sorted(Counter(axis_turns).items()):\n        pct = count / len(axis_turns) * 100\n        print(f\"  {t:<10} {count:>8} {pct:>7.1f}%\")\n    \n    # By category\n    short = sum(1 for t in axis_turns if t <= 3)\n    medium = sum(1 for t in axis_turns if 4 <= t <= 5)\n    long = sum(1 for t in axis_turns if t >= 6)\n    print(f\"\\n  Category breakdown:\")\n    print(f\"    Short (2-3):  {short:3d} ({short/len(axis_turns)*100:.1f}%)\")\n    print(f\"    Medium (4-5): {medium:3d} ({medium/len(axis_turns)*100:.1f}%)\")\n    print(f\"    Long (6+):    {long:3d} ({long/len(axis_turns)*100:.1f}%)\")\n\n# LaTeX output for turn by axis\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"LaTeX: Turn Composition by Axis\")\nprint(\"=\" * 60)\nprint(\"\\\\begin{tabular}{@{}lcccc@{}}\")\nprint(\"\\\\toprule\")\nprint(\"\\\\textbf{Axis} & \\\\textbf{n} & \\\\textbf{Short} & \\\\textbf{Medium} & \\\\textbf{Long} \\\\\\\\\")\nprint(\"\\\\midrule\")\n\nfor axis in ['INFERENCE_MEMORY', 'INSTRUCTION_RETENTION']:\n    axis_data = [d for d in data if d['AXIS'] == axis]\n    axis_turns = [len(d['CONVERSATION']) for d in axis_data]\n    \n    short = sum(1 for t in axis_turns if t <= 3)\n    medium = sum(1 for t in axis_turns if 4 <= t <= 5)\n    long = sum(1 for t in axis_turns if t >= 6)\n    \n    axis_display = axis.replace('_', '\\\\_')\n    print(f\"{axis_display} & {len(axis_data)} & {short} & {medium} & {long} \\\\\\\\\")\n\nprint(\"\\\\bottomrule\")\nprint(\"\\\\end{tabular}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Language Eval Results with Turn Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_responses_with_turns(model, condition):\n",
    "    \"\"\"Load responses and extract turn counts.\"\"\"\n",
    "    pattern = f'results/responses/{model}/responses_{condition}_*.jsonl'\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        return None\n",
    "    \n",
    "    data = []\n",
    "    with open(files[-1]) as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "            if item.get('success'):\n",
    "                data.append({\n",
    "                    'question_id': item.get('question_id'),\n",
    "                    'turn_count': item.get('turn_count'),\n",
    "                    'response': item.get('response', '')\n",
    "                })\n",
    "    return data\n",
    "\n",
    "def load_language_eval(model, condition):\n",
    "    \"\"\"Load language evaluation results.\"\"\"\n",
    "    pattern = f'results/layer1/{model}/language_eval_{condition}_*.jsonl'\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        return None\n",
    "    \n",
    "    data = {}\n",
    "    with open(files[-1]) as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "            qid = item.get('question_id')\n",
    "            data[qid] = item.get('match_status') == 'match'\n",
    "    return data\n",
    "\n",
    "# Test\n",
    "responses = load_responses_with_turns('gpt-5', 'de_to_en')\n",
    "lang_eval = load_language_eval('gpt-5', 'de_to_en')\n",
    "if responses and lang_eval:\n",
    "    print(f\"Loaded {len(responses)} responses, {len(lang_eval)} eval results\")\n",
    "else:\n",
    "    print(\"Data not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Fidelity by Length for X→EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fidelity_by_length(model):\n",
    "    \"\"\"Compute X→EN fidelity by conversation length.\"\"\"\n",
    "    results = {'Short': {'match': 0, 'total': 0},\n",
    "               'Medium': {'match': 0, 'total': 0},\n",
    "               'Long': {'match': 0, 'total': 0}}\n",
    "    \n",
    "    for lang in LANGS:\n",
    "        condition = f\"{lang}_to_en\"\n",
    "        \n",
    "        responses = load_responses_with_turns(model, condition)\n",
    "        lang_eval = load_language_eval(model, condition)\n",
    "        \n",
    "        if not responses or not lang_eval:\n",
    "            continue\n",
    "        \n",
    "        for resp in responses:\n",
    "            qid = resp['question_id']\n",
    "            turns = resp['turn_count']\n",
    "            category = categorize_length(turns)\n",
    "            \n",
    "            if qid in lang_eval:\n",
    "                results[category]['total'] += 1\n",
    "                if lang_eval[qid]:\n",
    "                    results[category]['match'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compute for all models\n",
    "all_results = {}\n",
    "for model_id, model_name in MODELS.items():\n",
    "    all_results[model_id] = compute_fidelity_by_length(model_id)\n",
    "\n",
    "print(\"X→EN Fidelity by Conversation Length\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<20} {'Short':>12} {'Medium':>12} {'Long':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    res = all_results[model_id]\n",
    "    row = []\n",
    "    for cat in ['Short', 'Medium', 'Long']:\n",
    "        if res[cat]['total'] > 0:\n",
    "            pct = res[cat]['match'] / res[cat]['total'] * 100\n",
    "            row.append(f\"{pct:.1f}% ({res[cat]['match']}/{res[cat]['total']})\")\n",
    "        else:\n",
    "            row.append(\"--\")\n",
    "    print(f\"{model_name:<20} {row[0]:>12} {row[1]:>12} {row[2]:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chi-Square Tests for Length Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(results):\n",
    "    \"\"\"Run chi-square test for independence between length and fidelity.\"\"\"\n",
    "    # Build contingency table: [[match_short, match_med, match_long], [mismatch_short, ...]]\n",
    "    observed = []\n",
    "    for cat in ['Short', 'Medium', 'Long']:\n",
    "        match = results[cat]['match']\n",
    "        total = results[cat]['total']\n",
    "        if total == 0:\n",
    "            return None, None\n",
    "        observed.append([match, total - match])\n",
    "    \n",
    "    observed = np.array(observed).T  # Transpose to get 2xN matrix\n",
    "    \n",
    "    # Check if test is valid (expected counts > 5)\n",
    "    if np.any(observed.sum(axis=0) < 5):\n",
    "        return None, \"Low counts\"\n",
    "    \n",
    "    try:\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(observed)\n",
    "        return chi2, p\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "print(\"Chi-Square Tests for Length Effect (X→EN)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<25} {'χ²':>10} {'p-value':>15} {'Significant?':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    res = all_results[model_id]\n",
    "    chi2, p = chi_square_test(res)\n",
    "    \n",
    "    if chi2 is not None and p is not None:\n",
    "        sig = \"Yes (p<0.001)\" if p < 0.001 else (\"Yes (p<0.05)\" if p < 0.05 else \"No\")\n",
    "        p_str = f\"<0.001\" if p < 0.001 else f\"{p:.3f}\"\n",
    "        print(f\"{model_name:<25} {chi2:>10.2f} {p_str:>15} {sig:>15}\")\n",
    "    elif p == \"Low counts\":\n",
    "        print(f\"{model_name:<25} {'--':>10} {'Low counts':>15} {'--':>15}\")\n",
    "    else:\n",
    "        print(f\"{model_name:<25} {'--':>10} {'--':>15} {'--':>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Breakdown by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fidelity_by_length_and_lang(model, lang):\n",
    "    \"\"\"Compute fidelity by length for specific language.\"\"\"\n",
    "    results = {'Short': {'match': 0, 'total': 0},\n",
    "               'Medium': {'match': 0, 'total': 0},\n",
    "               'Long': {'match': 0, 'total': 0}}\n",
    "    \n",
    "    condition = f\"{lang}_to_en\"\n",
    "    \n",
    "    responses = load_responses_with_turns(model, condition)\n",
    "    lang_eval = load_language_eval(model, condition)\n",
    "    \n",
    "    if not responses or not lang_eval:\n",
    "        return results\n",
    "    \n",
    "    for resp in responses:\n",
    "        qid = resp['question_id']\n",
    "        turns = resp['turn_count']\n",
    "        category = categorize_length(turns)\n",
    "        \n",
    "        if qid in lang_eval:\n",
    "            results[category]['total'] += 1\n",
    "            if lang_eval[qid]:\n",
    "                results[category]['match'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Show breakdown for a specific model\n",
    "print(\"Detailed Breakdown: Claude Opus 4.5 (X→EN)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for lang in LANGS:\n",
    "    res = compute_fidelity_by_length_and_lang('claude-opus-4.5', lang)\n",
    "    print(f\"\\n{lang.upper()}→EN:\")\n",
    "    for cat in ['Short', 'Medium', 'Long']:\n",
    "        if res[cat]['total'] > 0:\n",
    "            pct = res[cat]['match'] / res[cat]['total'] * 100\n",
    "            print(f\"  {cat}: {pct:.1f}% ({res[cat]['match']}/{res[cat]['total']})\")\n",
    "        else:\n",
    "            print(f\"  {cat}: --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LaTeX Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LaTeX: X→EN Fidelity by Conversation Length\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    res = all_results[model_id]\n",
    "    chi2, p = chi_square_test(res)\n",
    "    \n",
    "    vals = []\n",
    "    for cat in ['Short', 'Medium', 'Long']:\n",
    "        if res[cat]['total'] > 0:\n",
    "            pct = res[cat]['match'] / res[cat]['total'] * 100\n",
    "            vals.append(f\"{pct:.1f}\\\\%\")\n",
    "        else:\n",
    "            vals.append(\"--\")\n",
    "    \n",
    "    if p is not None and isinstance(p, float):\n",
    "        p_str = \"$<$0.001\" if p < 0.001 else f\"{p:.2f}\"\n",
    "    else:\n",
    "        p_str = \"—\"\n",
    "    \n",
    "    print(f\"{model_name} & {vals[0]} & {vals[1]} & {vals[2]} & {p_str} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for plotting\n",
    "categories = ['Short\\n(2-3)', 'Medium\\n(4-5)', 'Long\\n(6+)']\n",
    "x = np.arange(len(categories))\n",
    "width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, (model_id, model_name) in enumerate(MODELS.items()):\n",
    "    res = all_results[model_id]\n",
    "    vals = []\n",
    "    for cat in ['Short', 'Medium', 'Long']:\n",
    "        if res[cat]['total'] > 0:\n",
    "            vals.append(res[cat]['match'] / res[cat]['total'] * 100)\n",
    "        else:\n",
    "            vals.append(0)\n",
    "    \n",
    "    ax.bar(x + i * width, vals, width, label=model_name)\n",
    "\n",
    "ax.set_ylabel('Language Fidelity (%)')\n",
    "ax.set_xlabel('Conversation Length')\n",
    "ax.set_title('X→EN Fidelity by Conversation Length')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 105)\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Axis Breakdown (INFERENCE_MEMORY vs INSTRUCTION_RETENTION)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load baseline data with axis info\nbaseline_data = {}\nwith open('data/experiments/baseline_en.jsonl') as f:\n    for line in f:\n        item = json.loads(line.strip())\n        baseline_data[item['QUESTION_ID']] = {\n            'axis': item['AXIS'],\n            'turns': len(item['CONVERSATION'])\n        }\n\ndef compute_fidelity_by_axis_and_length(model):\n    \"\"\"Compute X→EN fidelity by axis and conversation length.\"\"\"\n    results = {\n        'INFERENCE_MEMORY': {'Short': {'match': 0, 'total': 0}, \n                             'Medium': {'match': 0, 'total': 0}, \n                             'Long': {'match': 0, 'total': 0}},\n        'INSTRUCTION_RETENTION': {'Short': {'match': 0, 'total': 0}, \n                                   'Medium': {'match': 0, 'total': 0}, \n                                   'Long': {'match': 0, 'total': 0}}\n    }\n    \n    for lang in LANGS:\n        condition = f\"{lang}_to_en\"\n        \n        responses = load_responses_with_turns(model, condition)\n        lang_eval = load_language_eval(model, condition)\n        \n        if not responses or not lang_eval:\n            continue\n        \n        for resp in responses:\n            qid = resp['question_id']\n            if qid not in baseline_data or qid not in lang_eval:\n                continue\n                \n            axis = baseline_data[qid]['axis']\n            turns = resp['turn_count']\n            category = categorize_length(turns)\n            \n            results[axis][category]['total'] += 1\n            if lang_eval[qid]:\n                results[axis][category]['match'] += 1\n    \n    return results\n\n# Compute axis breakdown for all models\naxis_results = {}\nfor model_id in MODELS.keys():\n    axis_results[model_id] = compute_fidelity_by_axis_and_length(model_id)\n\n# Display results\nprint(\"X→EN Fidelity by Axis and Conversation Length\")\nprint(\"=\" * 90)\n\nfor model_id, model_name in MODELS.items():\n    print(f\"\\n{model_name}:\")\n    print(f\"  {'Axis':<25} {'Short':>15} {'Medium':>15} {'Long':>15}\")\n    print(\"-\" * 75)\n    \n    for axis in ['INFERENCE_MEMORY', 'INSTRUCTION_RETENTION']:\n        res = axis_results[model_id][axis]\n        row = []\n        for cat in ['Short', 'Medium', 'Long']:\n            if res[cat]['total'] > 0:\n                pct = res[cat]['match'] / res[cat]['total'] * 100\n                row.append(f\"{pct:.1f}% ({res[cat]['match']}/{res[cat]['total']})\")\n            else:\n                row.append(\"--\")\n        print(f\"  {axis:<25} {row[0]:>15} {row[1]:>15} {row[2]:>15}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Axis Breakdown LaTeX Output",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# LaTeX output for axis breakdown\nprint(\"LaTeX: Axis Breakdown by Conversation Length\")\nprint(\"=\"*80)\n\n# First, show dataset composition\nprint(\"\\\\midrule\")\nprint(\"\\\\multicolumn{5}{l}{\\\\textit{Dataset Composition}} \\\\\\\\\")\nprint(\"\\\\midrule\")\n\n# Count samples by axis and length\naxis_counts = {'INFERENCE_MEMORY': {'Short': 0, 'Medium': 0, 'Long': 0},\n               'INSTRUCTION_RETENTION': {'Short': 0, 'Medium': 0, 'Long': 0}}\n\nfor qid, info in baseline_data.items():\n    axis = info['axis']\n    turns = info['turns']\n    cat = categorize_length(turns)\n    axis_counts[axis][cat] += 1\n\nfor axis in ['INFERENCE_MEMORY', 'INSTRUCTION_RETENTION']:\n    total = sum(axis_counts[axis].values())\n    counts = [str(axis_counts[axis][cat]) for cat in ['Short', 'Medium', 'Long']]\n    axis_display = axis.replace('_', '\\\\_')\n    print(f\"{axis_display} & {total} & {' & '.join(counts)} \\\\\\\\\")\n\n# Show fidelity by axis for each model\nprint()\nfor model_id, model_name in MODELS.items():\n    print(f\"\\\\midrule\")\n    print(f\"\\\\multicolumn{{5}}{{l}}{{\\\\textit{{{model_name}}}}} \\\\\\\\\")\n    print(f\"\\\\midrule\")\n    \n    for axis in ['INFERENCE_MEMORY', 'INSTRUCTION_RETENTION']:\n        res = axis_results[model_id][axis]\n        vals = []\n        for cat in ['Short', 'Medium', 'Long']:\n            if res[cat]['total'] > 0:\n                pct = res[cat]['match'] / res[cat]['total'] * 100\n                vals.append(f\"{pct:.1f}\\\\%\")\n            else:\n                vals.append(\"--\")\n        \n        # Compute overall for this axis\n        total_match = sum(res[cat]['match'] for cat in ['Short', 'Medium', 'Long'])\n        total_n = sum(res[cat]['total'] for cat in ['Short', 'Medium', 'Long'])\n        overall = f\"{total_match/total_n*100:.1f}\\\\%\" if total_n > 0 else \"--\"\n        \n        axis_display = axis.replace('_', '\\\\_')\n        print(f\"{axis_display} & {overall} & {' & '.join(vals)} \\\\\\\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}