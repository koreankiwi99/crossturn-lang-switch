{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Conversation Length Effect Analysis\n\nAnalyzes whether context-anchoring intensifies with conversation length.\n\n**Length categories:**\n- Short: 3-5 turns (n=44)\n- Medium: 7-9 turns (n=99)\n- Long: 11+ turns (n=39)\n\n**Focus:** X→EN fidelity (where context-anchoring is most visible)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom scipy import stats\n\n# Change to project root\nos.chdir(Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent)\nprint(f\"Working directory: {os.getcwd()}\")\n\nMODELS = {\n    'gpt-5': 'GPT-5',\n    'gemini-3-pro': 'Gemini 3 Pro',\n    'claude-opus-4.5': 'Claude Opus 4.5',\n    'deepseek-v3.1': 'DeepSeek-V3.1',\n    'command-r-plus': 'Command R+',\n}\n\nLANGS = ['de', 'zh', 'es', 'ar']\n\ndef categorize_length(turns):\n    \"\"\"Categorize conversation length.\"\"\"\n    if 3 <= turns <= 5:\n        return 'Short'\n    elif 7 <= turns <= 9:\n        return 'Medium'\n    elif turns >= 11:\n        return 'Long'\n    else:\n        return None  # Exclude 2, 6, 10 turns\n\nprint(f\"Models: {list(MODELS.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Turn Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze turn distribution in baseline dataset\nfrom collections import Counter\n\nwith open('data/experiments/baseline_en.jsonl') as f:\n    data = [json.loads(l) for l in f]\n    turns = [len(d['CONVERSATION']) for d in data]\n    axes = [d['AXIS'] for d in data]\n\nprint(\"Turn Distribution in Dataset:\")\nprint(\"=\" * 40)\nfor t, count in sorted(Counter(turns).items()):\n    pct = count / len(turns) * 100\n    cat = categorize_length(t)\n    cat_str = f\"({cat})\" if cat else \"(excluded)\"\n    print(f\"  {t:2d} turns: {count:3d} ({pct:5.1f}%) {cat_str}\")\n\nprint(f\"\\nTotal samples: {len(turns)}\")\nprint(f\"\\nBy category:\")\nshort = sum(1 for t in turns if 3 <= t <= 5)\nmedium = sum(1 for t in turns if 7 <= t <= 9)\nlong = sum(1 for t in turns if t >= 11)\nprint(f\"  Short (3-5 turns):   {short:3d} ({short/len(turns)*100:.1f}%)\")\nprint(f\"  Medium (7-9 turns):  {medium:3d} ({medium/len(turns)*100:.1f}%)\")\nprint(f\"  Long (11+ turns):    {long:3d} ({long/len(turns)*100:.1f}%)\")\n\nprint(f\"\\nBy axis:\")\nfor axis, count in Counter(axes).items():\n    print(f\"  {axis}: {count}\")\n\n# Turn distribution by axis\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"Turn Distribution BY AXIS:\")\nprint(\"=\" * 60)\n\nfor axis in ['INFERENCE_MEMORY', 'INSTRUCTION_RETENTION']:\n    axis_data = [d for d in data if d['AXIS'] == axis]\n    axis_turns = [len(d['CONVERSATION']) for d in axis_data]\n    \n    print(f\"\\n{axis} (n={len(axis_data)}):\")\n    print(f\"  {'Turns':<10} {'Count':>8} {'Pct':>8}\")\n    print(\"-\" * 30)\n    for t, count in sorted(Counter(axis_turns).items()):\n        pct = count / len(axis_turns) * 100\n        print(f\"  {t:<10} {count:>8} {pct:>7.1f}%\")\n    \n    # By category\n    short = sum(1 for t in axis_turns if 3 <= t <= 5)\n    medium = sum(1 for t in axis_turns if 7 <= t <= 9)\n    long = sum(1 for t in axis_turns if t >= 11)\n    print(f\"\\n  Category breakdown:\")\n    print(f\"    Short (3-5):   {short:3d} ({short/len(axis_turns)*100:.1f}%)\")\n    print(f\"    Medium (7-9):  {medium:3d} ({medium/len(axis_turns)*100:.1f}%)\")\n    print(f\"    Long (11+):    {long:3d} ({long/len(axis_turns)*100:.1f}%)\")\n\n# LaTeX output for turn by axis\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"LaTeX: Turn Composition by Axis\")\nprint(\"=\" * 60)\nprint(\"\\\\begin{tabular}{@{}lcccc@{}}\")\nprint(\"\\\\toprule\")\nprint(\"\\\\textbf{Axis} & \\\\textbf{n} & \\\\textbf{Short} & \\\\textbf{Medium} & \\\\textbf{Long} \\\\\\\\\")\nprint(\"\\\\midrule\")\n\nfor axis in ['INFERENCE_MEMORY', 'INSTRUCTION_RETENTION']:\n    axis_data = [d for d in data if d['AXIS'] == axis]\n    axis_turns = [len(d['CONVERSATION']) for d in axis_data]\n    \n    short = sum(1 for t in axis_turns if 3 <= t <= 5)\n    medium = sum(1 for t in axis_turns if 7 <= t <= 9)\n    long = sum(1 for t in axis_turns if t >= 11)\n    \n    axis_display = axis.replace('_', '\\\\_')\n    print(f\"{axis_display} & {len(axis_data)} & {short} & {medium} & {long} \\\\\\\\\")\n\nprint(\"\\\\bottomrule\")\nprint(\"\\\\end{tabular}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Language Eval Results with Turn Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_responses_with_turns(model, condition):\n    \"\"\"Load responses and extract turn counts.\"\"\"\n    pattern = f'results/responses/{model}/responses_{condition}_*.jsonl'\n    files = sorted(glob.glob(pattern))\n    # Exclude variance runs (run2, run3) - only use primary results\n    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n    if not files:\n        return None\n    \n    data = []\n    with open(files[-1]) as f:\n        for line in f:\n            item = json.loads(line.strip())\n            if item.get('success'):\n                data.append({\n                    'question_id': item.get('question_id'),\n                    'turn_count': item.get('turn_count'),\n                    'response': item.get('response', '')\n                })\n    return data\n\ndef load_language_eval(model, condition):\n    \"\"\"Load language evaluation results.\"\"\"\n    pattern = f'results/layer1/{model}/language_eval_{condition}_*.jsonl'\n    files = sorted(glob.glob(pattern))\n    # Exclude variance runs (run2, run3) - only use primary results\n    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n    if not files:\n        return None\n    \n    data = {}\n    with open(files[-1]) as f:\n        for line in f:\n            item = json.loads(line.strip())\n            qid = item.get('question_id')\n            data[qid] = item.get('match_status') == 'match'\n    return data\n\n# Test\nresponses = load_responses_with_turns('gpt-5', 'de_to_en')\nlang_eval = load_language_eval('gpt-5', 'de_to_en')\nif responses and lang_eval:\n    print(f\"Loaded {len(responses)} responses, {len(lang_eval)} eval results\")\nelse:\n    print(\"Data not found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Fidelity by Length for X→EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_fidelity_by_length(model):\n    \"\"\"Compute X->EN fidelity by conversation length.\"\"\"\n    results = {'Short': {'match': 0, 'total': 0},\n               'Medium': {'match': 0, 'total': 0},\n               'Long': {'match': 0, 'total': 0}}\n    \n    for lang in LANGS:\n        condition = f\"{lang}_to_en\"\n        \n        responses = load_responses_with_turns(model, condition)\n        lang_eval = load_language_eval(model, condition)\n        \n        if not responses or not lang_eval:\n            continue\n        \n        for resp in responses:\n            qid = resp['question_id']\n            turns = resp['turn_count']\n            category = categorize_length(turns)\n            \n            if category and qid in lang_eval:\n                results[category]['total'] += 1\n                if lang_eval[qid]:\n                    results[category]['match'] += 1\n    \n    return results\n\n# Compute for all models\nall_results = {}\nfor model_id, model_name in MODELS.items():\n    all_results[model_id] = compute_fidelity_by_length(model_id)\n\nprint(\"X->EN Fidelity by Conversation Length\")\nprint(\"=\" * 70)\nprint(f\"{'Model':<20} {'Short (3-5)':>15} {'Medium (7-9)':>15} {'Long (11+)':>15}\")\nprint(\"-\" * 70)\n\nfor model_id, model_name in MODELS.items():\n    res = all_results[model_id]\n    row = []\n    for cat in ['Short', 'Medium', 'Long']:\n        if res[cat]['total'] > 0:\n            pct = res[cat]['match'] / res[cat]['total'] * 100\n            row.append(f\"{pct:.1f}% ({res[cat]['match']}/{res[cat]['total']})\")\n        else:\n            row.append(\"--\")\n    print(f\"{model_name:<20} {row[0]:>15} {row[1]:>15} {row[2]:>15}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chi-Square Tests for Length Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(results):\n",
    "    \"\"\"Run chi-square test for independence between length and fidelity.\"\"\"\n",
    "    # Build contingency table: [[match_short, match_med, match_long], [mismatch_short, ...]]\n",
    "    observed = []\n",
    "    for cat in ['Short', 'Medium', 'Long']:\n",
    "        match = results[cat]['match']\n",
    "        total = results[cat]['total']\n",
    "        if total == 0:\n",
    "            return None, None\n",
    "        observed.append([match, total - match])\n",
    "    \n",
    "    observed = np.array(observed).T  # Transpose to get 2xN matrix\n",
    "    \n",
    "    # Check if test is valid (expected counts > 5)\n",
    "    if np.any(observed.sum(axis=0) < 5):\n",
    "        return None, \"Low counts\"\n",
    "    \n",
    "    try:\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(observed)\n",
    "        return chi2, p\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "print(\"Chi-Square Tests for Length Effect (X→EN)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<25} {'χ²':>10} {'p-value':>15} {'Significant?':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    res = all_results[model_id]\n",
    "    chi2, p = chi_square_test(res)\n",
    "    \n",
    "    if chi2 is not None and p is not None:\n",
    "        sig = \"Yes (p<0.001)\" if p < 0.001 else (\"Yes (p<0.05)\" if p < 0.05 else \"No\")\n",
    "        p_str = f\"<0.001\" if p < 0.001 else f\"{p:.3f}\"\n",
    "        print(f\"{model_name:<25} {chi2:>10.2f} {p_str:>15} {sig:>15}\")\n",
    "    elif p == \"Low counts\":\n",
    "        print(f\"{model_name:<25} {'--':>10} {'Low counts':>15} {'--':>15}\")\n",
    "    else:\n",
    "        print(f\"{model_name:<25} {'--':>10} {'--':>15} {'--':>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Breakdown by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_fidelity_by_length_and_lang(model, lang):\n    \"\"\"Compute fidelity by length for specific language.\"\"\"\n    results = {'Short': {'match': 0, 'total': 0},\n               'Medium': {'match': 0, 'total': 0},\n               'Long': {'match': 0, 'total': 0}}\n    \n    condition = f\"{lang}_to_en\"\n    \n    responses = load_responses_with_turns(model, condition)\n    lang_eval = load_language_eval(model, condition)\n    \n    if not responses or not lang_eval:\n        return results\n    \n    for resp in responses:\n        qid = resp['question_id']\n        turns = resp['turn_count']\n        category = categorize_length(turns)\n        \n        if category and qid in lang_eval:\n            results[category]['total'] += 1\n            if lang_eval[qid]:\n                results[category]['match'] += 1\n    \n    return results\n\n# Show breakdown for a specific model\nprint(\"Detailed Breakdown: Claude Opus 4.5 (X->EN)\")\nprint(\"=\" * 60)\n\nfor lang in LANGS:\n    res = compute_fidelity_by_length_and_lang('claude-opus-4.5', lang)\n    print(f\"\\n{lang.upper()}->EN:\")\n    for cat in ['Short', 'Medium', 'Long']:\n        if res[cat]['total'] > 0:\n            pct = res[cat]['match'] / res[cat]['total'] * 100\n            print(f\"  {cat}: {pct:.1f}% ({res[cat]['match']}/{res[cat]['total']})\")\n        else:\n            print(f\"  {cat}: --\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LaTeX Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"LaTeX: X->EN Fidelity by Conversation Length\")\nprint(\"=\"*80)\n\nfor model_id, model_name in MODELS.items():\n    res = all_results[model_id]\n    chi2, p = chi_square_test(res)\n    \n    vals = []\n    for cat in ['Short', 'Medium', 'Long']:\n        if res[cat]['total'] > 0:\n            pct = res[cat]['match'] / res[cat]['total'] * 100\n            vals.append(f\"{pct:.1f}\\\\%\")\n        else:\n            vals.append(\"--\")\n    \n    if p is not None and isinstance(p, float):\n        p_str = \"$<$0.001\" if p < 0.001 else f\"{p:.2f}\"\n    else:\n        p_str = \"---\"\n    \n    print(f\"{model_name} & {vals[0]} & {vals[1]} & {vals[2]} & {p_str} \\\\\\\\\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Axis Breakdown (INFERENCE_MEMORY vs INSTRUCTION_RETENTION)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load baseline data with axis info\nbaseline_data = {}\nwith open('data/experiments/baseline_en.jsonl') as f:\n    for line in f:\n        item = json.loads(line.strip())\n        baseline_data[item['QUESTION_ID']] = {\n            'axis': item['AXIS'],\n            'turns': len(item['CONVERSATION'])\n        }\n\ndef compute_fidelity_by_axis_and_length(model):\n    \"\"\"Compute X->EN fidelity by axis and conversation length.\"\"\"\n    results = {\n        'INFERENCE_MEMORY': {'Short': {'match': 0, 'total': 0}, \n                             'Medium': {'match': 0, 'total': 0}, \n                             'Long': {'match': 0, 'total': 0}},\n        'INSTRUCTION_RETENTION': {'Short': {'match': 0, 'total': 0}, \n                                   'Medium': {'match': 0, 'total': 0}, \n                                   'Long': {'match': 0, 'total': 0}}\n    }\n    \n    for lang in LANGS:\n        condition = f\"{lang}_to_en\"\n        \n        responses = load_responses_with_turns(model, condition)\n        lang_eval = load_language_eval(model, condition)\n        \n        if not responses or not lang_eval:\n            continue\n        \n        for resp in responses:\n            qid = resp['question_id']\n            if qid not in baseline_data or qid not in lang_eval:\n                continue\n                \n            axis = baseline_data[qid]['axis']\n            turns = resp['turn_count']\n            category = categorize_length(turns)\n            \n            if category is None:\n                continue\n            \n            results[axis][category]['total'] += 1\n            if lang_eval[qid]:\n                results[axis][category]['match'] += 1\n    \n    return results\n\n# Compute axis breakdown for all models\naxis_results = {}\nfor model_id in MODELS.keys():\n    axis_results[model_id] = compute_fidelity_by_axis_and_length(model_id)\n\n# Display results\nprint(\"X->EN Fidelity by Axis and Conversation Length\")\nprint(\"=\" * 90)\n\nfor model_id, model_name in MODELS.items():\n    print(f\"\\n{model_name}:\")\n    print(f\"  {'Axis':<25} {'Short (3-5)':>15} {'Medium (7-9)':>15} {'Long (11+)':>15}\")\n    print(\"-\" * 75)\n    \n    for axis in ['INFERENCE_MEMORY', 'INSTRUCTION_RETENTION']:\n        res = axis_results[model_id][axis]\n        row = []\n        for cat in ['Short', 'Medium', 'Long']:\n            if res[cat]['total'] > 0:\n                pct = res[cat]['match'] / res[cat]['total'] * 100\n                row.append(f\"{pct:.1f}% ({res[cat]['match']}/{res[cat]['total']})\")\n            else:\n                row.append(\"--\")\n        print(f\"  {axis:<25} {row[0]:>15} {row[1]:>15} {row[2]:>15}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}