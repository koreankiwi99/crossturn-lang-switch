{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Run Consistency Analysis\n",
    "\n",
    "This notebook analyzes the reproducibility/consistency of model responses across 3 independent runs.\n",
    "\n",
    "**Condition**: `es_to_en` (Spanish context → English query)  \n",
    "**Questions**: 182 per run  \n",
    "**Models**: GPT-5, Gemini 3 Pro\n",
    "\n",
    "## Metrics Analyzed:\n",
    "- **Layer 1 (Language Fidelity)**: Does the model respond in the correct language?\n",
    "- **Layer 2 (Task Accuracy)**: Does the model complete the task correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For nice table display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for the 3 runs\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "\n",
    "# Layer 2: Task Accuracy files\n",
    "LAYER2_FILES = {\n",
    "    \"GPT-5\": {\n",
    "        \"run1\": RESULTS_DIR / \"layer2/gpt-5/evaluated_es_to_en_20251219_203007.jsonl\",\n",
    "        \"run2\": RESULTS_DIR / \"gpt-5/evaluated_es_to_en_variance_run2_20251220_005346.jsonl\",\n",
    "        \"run3\": RESULTS_DIR / \"gpt-5/evaluated_es_to_en_variance_run3_20251220_005409.jsonl\",\n",
    "    },\n",
    "    \"Gemini 3 Pro\": {\n",
    "        \"run1\": RESULTS_DIR / \"layer2/gemini-3-pro/evaluated_es_to_en_20251219_202518.jsonl\",\n",
    "        \"run2\": RESULTS_DIR / \"gemini-3-pro/evaluated_es_to_en_variance_run2_20251220_005112.jsonl\",\n",
    "        \"run3\": RESULTS_DIR / \"gemini-3-pro/evaluated_es_to_en_variance_run3_20251220_005133.jsonl\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Layer 1: Language Fidelity files (only Gemini has 3 runs)\n",
    "LAYER1_FILES = {\n",
    "    \"GPT-5\": {\n",
    "        \"run1\": RESULTS_DIR / \"layer1/gpt-5/language_eval_es_to_en_20251219_213343.jsonl\",\n",
    "    },\n",
    "    \"Gemini 3 Pro\": {\n",
    "        \"run1\": RESULTS_DIR / \"layer1/gemini-3-pro/language_eval_es_to_en_20251219_213507.jsonl\",\n",
    "        \"run2\": RESULTS_DIR / \"gemini-3-pro/language_eval_es_to_en_20251220_003921.jsonl\",\n",
    "        \"run3\": RESULTS_DIR / \"gemini-3-pro/language_eval_es_to_en_20251220_003926.jsonl\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer2_evaluations(filepath):\n",
    "    \"\"\"Load Layer 2 (Task Accuracy) evaluations.\"\"\"\n",
    "    results = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                record = json.loads(line)\n",
    "                qid = record.get(\"question_id\")\n",
    "                if qid:\n",
    "                    eval_data = record.get(\"evaluation\", {})\n",
    "                    results[qid] = {\n",
    "                        \"verdict\": eval_data.get(\"verdict\"),\n",
    "                        \"passed\": eval_data.get(\"passed\"),\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_layer1_evaluations(filepath):\n",
    "    \"\"\"Load Layer 1 (Language Fidelity) evaluations.\"\"\"\n",
    "    results = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                record = json.loads(line)\n",
    "                qid = record.get(\"question_id\")\n",
    "                if qid:\n",
    "                    results[qid] = {\n",
    "                        \"match_status\": record.get(\"match_status\"),\n",
    "                        \"detected_language\": record.get(\"detected_language\"),\n",
    "                        \"expected_language\": record.get(\"expected_language\"),\n",
    "                        \"is_match\": record.get(\"match_status\") == \"match\",\n",
    "                    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rate(results, key=\"passed\"):\n",
    "    \"\"\"Calculate success rate.\"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    passed = sum(1 for r in results.values() if r.get(key))\n",
    "    return passed / len(results) * 100\n",
    "\n",
    "\n",
    "def calculate_pairwise_agreement(results1, results2, key=\"passed\"):\n",
    "    \"\"\"Calculate pairwise agreement rate.\"\"\"\n",
    "    common_ids = set(results1.keys()) & set(results2.keys())\n",
    "    if not common_ids:\n",
    "        return 0.0\n",
    "    agreements = sum(1 for qid in common_ids \n",
    "                     if results1[qid].get(key) == results2[qid].get(key))\n",
    "    return agreements / len(common_ids) * 100\n",
    "\n",
    "\n",
    "def calculate_three_way_agreement(r1, r2, r3, key=\"passed\"):\n",
    "    \"\"\"Calculate three-way agreement rate.\"\"\"\n",
    "    common_ids = set(r1.keys()) & set(r2.keys()) & set(r3.keys())\n",
    "    if not common_ids:\n",
    "        return 0.0\n",
    "    all_agree = sum(1 for qid in common_ids \n",
    "                    if r1[qid].get(key) == r2[qid].get(key) == r3[qid].get(key))\n",
    "    return all_agree / len(common_ids) * 100\n",
    "\n",
    "\n",
    "def cohen_kappa(results1, results2, key=\"passed\"):\n",
    "    \"\"\"Calculate Cohen's Kappa.\"\"\"\n",
    "    common_ids = set(results1.keys()) & set(results2.keys())\n",
    "    if not common_ids:\n",
    "        return 0.0\n",
    "    \n",
    "    n11 = sum(1 for qid in common_ids if results1[qid].get(key) and results2[qid].get(key))\n",
    "    n00 = sum(1 for qid in common_ids if not results1[qid].get(key) and not results2[qid].get(key))\n",
    "    n10 = sum(1 for qid in common_ids if results1[qid].get(key) and not results2[qid].get(key))\n",
    "    n01 = sum(1 for qid in common_ids if not results1[qid].get(key) and results2[qid].get(key))\n",
    "    \n",
    "    n = len(common_ids)\n",
    "    p_o = (n11 + n00) / n\n",
    "    p_yes = ((n11 + n10) / n) * ((n11 + n01) / n)\n",
    "    p_no = ((n00 + n01) / n) * ((n00 + n10) / n)\n",
    "    p_e = p_yes + p_no\n",
    "    \n",
    "    if p_e == 1:\n",
    "        return 1.0\n",
    "    return (p_o - p_e) / (1 - p_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Task Accuracy Consistency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all Layer 2 data\n",
    "layer2_data = {}\n",
    "for model_name, run_files in LAYER2_FILES.items():\n",
    "    layer2_data[model_name] = {}\n",
    "    for run_name, filepath in run_files.items():\n",
    "        if filepath.exists():\n",
    "            layer2_data[model_name][run_name] = load_layer2_evaluations(filepath)\n",
    "            print(f\"{model_name} {run_name}: {len(layer2_data[model_name][run_name])} records\")\n",
    "        else:\n",
    "            print(f\"{model_name} {run_name}: FILE NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Layer 2 metrics for each model\n",
    "layer2_results = []\n",
    "\n",
    "for model_name, runs_data in layer2_data.items():\n",
    "    if len(runs_data) < 3:\n",
    "        continue\n",
    "    \n",
    "    # Pass rates\n",
    "    rates = [calculate_rate(runs_data[f\"run{i}\"]) for i in range(1, 4)]\n",
    "    mean_rate = np.mean(rates)\n",
    "    std_rate = np.std(rates)\n",
    "    cv = (std_rate / mean_rate * 100) if mean_rate > 0 else 0\n",
    "    \n",
    "    # Pairwise agreement\n",
    "    pairs = [(\"run1\", \"run2\"), (\"run1\", \"run3\"), (\"run2\", \"run3\")]\n",
    "    agreements = [calculate_pairwise_agreement(runs_data[r1], runs_data[r2]) for r1, r2 in pairs]\n",
    "    kappas = [cohen_kappa(runs_data[r1], runs_data[r2]) for r1, r2 in pairs]\n",
    "    \n",
    "    # Three-way agreement\n",
    "    three_way = calculate_three_way_agreement(runs_data[\"run1\"], runs_data[\"run2\"], runs_data[\"run3\"])\n",
    "    \n",
    "    layer2_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Run 1 (%)\": rates[0],\n",
    "        \"Run 2 (%)\": rates[1],\n",
    "        \"Run 3 (%)\": rates[2],\n",
    "        \"Mean (%)\": mean_rate,\n",
    "        \"Std Dev\": std_rate,\n",
    "        \"CV (%)\": cv,\n",
    "        \"Pairwise Agr. (%)\": np.mean(agreements),\n",
    "        \"Cohen's κ\": np.mean(kappas),\n",
    "        \"3-Way Agr. (%)\": three_way,\n",
    "    })\n",
    "\n",
    "layer2_df = pd.DataFrame(layer2_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 1: Layer 2 (Task Accuracy) Consistency Across 3 Runs\")\n",
    "print(\"=\"*80)\n",
    "layer2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice formatted display\n",
    "layer2_display = layer2_df.copy()\n",
    "layer2_display = layer2_display.round(2)\n",
    "\n",
    "# Add mean row\n",
    "mean_row = layer2_display.select_dtypes(include=[np.number]).mean()\n",
    "mean_row[\"Model\"] = \"Mean\"\n",
    "layer2_display = pd.concat([layer2_display, pd.DataFrame([mean_row])], ignore_index=True)\n",
    "\n",
    "layer2_display.style.set_caption(\"Layer 2 (Task Accuracy) Consistency\").format(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1: Language Fidelity Consistency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all Layer 1 data\n",
    "layer1_data = {}\n",
    "for model_name, run_files in LAYER1_FILES.items():\n",
    "    layer1_data[model_name] = {}\n",
    "    for run_name, filepath in run_files.items():\n",
    "        if filepath.exists():\n",
    "            layer1_data[model_name][run_name] = load_layer1_evaluations(filepath)\n",
    "            print(f\"{model_name} {run_name}: {len(layer1_data[model_name][run_name])} records\")\n",
    "        else:\n",
    "            print(f\"{model_name} {run_name}: FILE NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Layer 1 metrics (only for models with 3 runs)\n",
    "layer1_results = []\n",
    "\n",
    "for model_name, runs_data in layer1_data.items():\n",
    "    num_runs = len(runs_data)\n",
    "    \n",
    "    # Fidelity rates\n",
    "    rates = [calculate_rate(runs_data[f\"run{i}\"], key=\"is_match\") for i in range(1, num_runs + 1)]\n",
    "    \n",
    "    if num_runs >= 3:\n",
    "        mean_rate = np.mean(rates)\n",
    "        std_rate = np.std(rates)\n",
    "        cv = (std_rate / mean_rate * 100) if mean_rate > 0 else 0\n",
    "        \n",
    "        # Pairwise agreement\n",
    "        pairs = [(\"run1\", \"run2\"), (\"run1\", \"run3\"), (\"run2\", \"run3\")]\n",
    "        agreements = [calculate_pairwise_agreement(runs_data[r1], runs_data[r2], key=\"is_match\") for r1, r2 in pairs]\n",
    "        kappas = [cohen_kappa(runs_data[r1], runs_data[r2], key=\"is_match\") for r1, r2 in pairs]\n",
    "        \n",
    "        # Three-way agreement\n",
    "        three_way = calculate_three_way_agreement(\n",
    "            runs_data[\"run1\"], runs_data[\"run2\"], runs_data[\"run3\"], key=\"is_match\"\n",
    "        )\n",
    "        \n",
    "        layer1_results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Run 1 (%)\": rates[0],\n",
    "            \"Run 2 (%)\": rates[1],\n",
    "            \"Run 3 (%)\": rates[2],\n",
    "            \"Mean (%)\": mean_rate,\n",
    "            \"Std Dev\": std_rate,\n",
    "            \"CV (%)\": cv,\n",
    "            \"Pairwise Agr. (%)\": np.mean(agreements),\n",
    "            \"Cohen's κ\": np.mean(kappas),\n",
    "            \"3-Way Agr. (%)\": three_way,\n",
    "        })\n",
    "    else:\n",
    "        # Only 1 run available\n",
    "        layer1_results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Run 1 (%)\": rates[0] if len(rates) > 0 else None,\n",
    "            \"Run 2 (%)\": rates[1] if len(rates) > 1 else None,\n",
    "            \"Run 3 (%)\": rates[2] if len(rates) > 2 else None,\n",
    "            \"Mean (%)\": rates[0] if len(rates) == 1 else np.mean(rates),\n",
    "            \"Std Dev\": None,\n",
    "            \"CV (%)\": None,\n",
    "            \"Pairwise Agr. (%)\": None,\n",
    "            \"Cohen's κ\": None,\n",
    "            \"3-Way Agr. (%)\": None,\n",
    "        })\n",
    "\n",
    "layer1_df = pd.DataFrame(layer1_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 2: Layer 1 (Language Fidelity) Consistency Across Runs\")\n",
    "print(\"=\"*80)\n",
    "layer1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined summary\n",
    "summary_data = []\n",
    "\n",
    "for model in [\"GPT-5\", \"Gemini 3 Pro\"]:\n",
    "    row = {\"Model\": model}\n",
    "    \n",
    "    # Layer 2 metrics\n",
    "    l2 = layer2_df[layer2_df[\"Model\"] == model]\n",
    "    if not l2.empty:\n",
    "        row[\"L2 Mean (%)\"] = l2[\"Mean (%)\"].values[0]\n",
    "        row[\"L2 Std Dev\"] = l2[\"Std Dev\"].values[0]\n",
    "        row[\"L2 3-Way Agr. (%)\"] = l2[\"3-Way Agr. (%)\"].values[0]\n",
    "        row[\"L2 κ\"] = l2[\"Cohen's κ\"].values[0]\n",
    "    \n",
    "    # Layer 1 metrics\n",
    "    l1 = layer1_df[layer1_df[\"Model\"] == model]\n",
    "    if not l1.empty:\n",
    "        row[\"L1 Mean (%)\"] = l1[\"Mean (%)\"].values[0]\n",
    "        row[\"L1 Std Dev\"] = l1[\"Std Dev\"].values[0]\n",
    "        row[\"L1 3-Way Agr. (%)\"] = l1[\"3-Way Agr. (%)\"].values[0]\n",
    "        row[\"L1 κ\"] = l1[\"Cohen's κ\"].values[0]\n",
    "    \n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 3: Combined Layer 1 & Layer 2 Consistency Summary\")\n",
    "print(\"=\"*80)\n",
    "summary_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_patterns(runs_data, key=\"passed\"):\n",
    "    \"\"\"Analyze response patterns across runs.\"\"\"\n",
    "    common_ids = set.intersection(*[set(r.keys()) for r in runs_data.values()])\n",
    "    \n",
    "    patterns = defaultdict(int)\n",
    "    for qid in common_ids:\n",
    "        vals = tuple(runs_data[run][qid].get(key) for run in sorted(runs_data.keys()))\n",
    "        patterns[vals] += 1\n",
    "    \n",
    "    # Aggregate patterns\n",
    "    pattern_map = {\n",
    "        (True, True, True): \"All PASS (3/3)\",\n",
    "        (False, False, False): \"All FAIL (3/3)\",\n",
    "    }\n",
    "    for p in [(True, True, False), (True, False, True), (False, True, True)]:\n",
    "        pattern_map[p] = \"2 PASS, 1 FAIL\"\n",
    "    for p in [(False, False, True), (False, True, False), (True, False, False)]:\n",
    "        pattern_map[p] = \"1 PASS, 2 FAIL\"\n",
    "    \n",
    "    aggregated = defaultdict(int)\n",
    "    for pattern, count in patterns.items():\n",
    "        label = pattern_map.get(pattern, str(pattern))\n",
    "        aggregated[label] += count\n",
    "    \n",
    "    return dict(aggregated), len(common_ids)\n",
    "\n",
    "\n",
    "# Layer 2 patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 4: Response Pattern Distribution (Layer 2 - Task Accuracy)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pattern_data = []\n",
    "for model_name, runs_data in layer2_data.items():\n",
    "    if len(runs_data) >= 3:\n",
    "        patterns, total = analyze_patterns(runs_data)\n",
    "        for pattern, count in sorted(patterns.items(), key=lambda x: -x[1]):\n",
    "            pattern_data.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Pattern\": pattern,\n",
    "                \"Count\": count,\n",
    "                \"Percentage\": count / total * 100,\n",
    "            })\n",
    "\n",
    "pattern_df = pd.DataFrame(pattern_data)\n",
    "pattern_pivot = pattern_df.pivot(index=\"Pattern\", columns=\"Model\", values=\"Percentage\").fillna(0)\n",
    "pattern_pivot.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LaTeX Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table\n",
    "print(r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\caption{Response consistency across 3 independent runs on 182 es\\_to\\_en questions.}\n",
    "\\label{tab:consistency}\n",
    "\\begin{tabular}{lcccccc}\n",
    "\\toprule\n",
    "\\textbf{Model} & \\textbf{Run 1} & \\textbf{Run 2} & \\textbf{Run 3} & \\textbf{Mean} & \\textbf{Std Dev} & \\textbf{3-Way Agr.} \\\\\n",
    " & (\\%) & (\\%) & (\\%) & (\\%) & (\\%) & (\\%) \\\\\n",
    "\\midrule\"\"\")\n",
    "\n",
    "for _, row in layer2_df.iterrows():\n",
    "    print(f\"{row['Model']} & {row['Run 1 (%)']:.2f} & {row['Run 2 (%)']:.2f} & {row['Run 3 (%)']:.2f} & {row['Mean (%)']:.2f} & {row['Std Dev']:.2f} & {row['3-Way Agr. (%)']:.2f} \\\\\\\\\")\n",
    "\n",
    "# Mean row\n",
    "means = layer2_df.select_dtypes(include=[np.number]).mean()\n",
    "print(r\"\\midrule\")\n",
    "print(f\"\\\\textbf{{Mean}} & {means['Run 1 (%)']:.2f} & {means['Run 2 (%)']:.2f} & {means['Run 3 (%)']:.2f} & {means['Mean (%)']:.2f} & {means['Std Dev']:.2f} & {means['3-Way Agr. (%)']:.2f} \\\\\\\\\")\n",
    "\n",
    "print(r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Pass Rate Stability**:\n",
    "   - **GPT-5**: Very high stability (CV ~1%), nearly identical results across runs\n",
    "   - **Gemini 3 Pro**: Moderate stability (CV ~4%), more variance between runs\n",
    "\n",
    "2. **Inter-Run Agreement**:\n",
    "   - **Cohen's κ**: 0.55-0.64 indicates moderate to substantial agreement\n",
    "   - **3-Way Agreement**: 66-77% of questions get identical verdicts across all 3 runs\n",
    "\n",
    "3. **Interpretation Guide**:\n",
    "   - CV (Coefficient of Variation): Lower is better. <5% indicates high stability\n",
    "   - Cohen's κ: 0.61-0.80 = Substantial, 0.41-0.60 = Moderate agreement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
