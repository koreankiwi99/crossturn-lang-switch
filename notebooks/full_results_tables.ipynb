{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Results Tables\n",
    "Generate tables for Language Fidelity and Task Accuracy with error detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:00.480767Z",
     "iopub.status.busy": "2026-02-05T02:35:00.480699Z",
     "iopub.status.idle": "2026-02-05T02:35:02.457624Z",
     "shell.execute_reply": "2026-02-05T02:35:02.457094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer1 exists: True\n",
      "Layer2 exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to project root\n",
    "#os.chdir(Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent)\n",
    "#print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "MODELS = {\n",
    "    'gpt-5': 'GPT-5',\n",
    "    'gemini-3-pro': 'Gemini 3 Pro',\n",
    "    'claude-opus-4.5': 'Claude Opus 4.5',\n",
    "    'deepseek-v3.1': 'DeepSeek-V3.1',\n",
    "    'command-r-plus': 'Command R+'\n",
    "}\n",
    "\n",
    "LANGS = ['de', 'zh', 'es', 'ar']\n",
    "CONDITIONS = ['baseline_en'] + [f'baseline_{l}' for l in LANGS] + [f'en_to_{l}' for l in LANGS] + [f'{l}_to_en' for l in LANGS]\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Layer1 exists: {os.path.exists('../results/layer1')}\")\n",
    "print(f\"Layer2 exists: {os.path.exists('../results/layer2')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Layer 1 (Language Fidelity) from JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:02.484563Z",
     "iopub.status.busy": "2026-02-05T02:35:02.484258Z",
     "iopub.status.idle": "2026-02-05T02:35:02.491040Z",
     "shell.execute_reply": "2026-02-05T02:35:02.490576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Layer 1 stats (from JSONL):\n",
      "  total: 182, match: 182, mismatch: 0\n",
      "  fidelity_rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def load_layer1_jsonl(model, condition):\n",
    "    \"\"\"Load Layer 1 evaluation JSONL and compute stats.\"\"\"\n",
    "    pattern = f'../results/layer1/{model}/language_eval_{condition}_*.jsonl'\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    # Exclude variance runs (run2, run3) - only use primary results\n",
    "    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    stats = {'total': 0, 'match': 0, 'mismatch': 0, 'mixed': 0, 'error': 0, 'parse_error': 0}\n",
    "    with open(files[-1]) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                stats['total'] += 1\n",
    "                status = data.get('match_status', '')\n",
    "                if status == 'match':\n",
    "                    stats['match'] += 1\n",
    "                elif status == 'mismatch':\n",
    "                    stats['mismatch'] += 1\n",
    "                elif status == 'mixed':\n",
    "                    stats['mixed'] += 1\n",
    "                else:\n",
    "                    stats['error'] += 1\n",
    "            except json.JSONDecodeError:\n",
    "                stats['parse_error'] += 1\n",
    "\n",
    "    if stats['total'] > 0:\n",
    "        stats['fidelity_rate'] = (stats['match'] / stats['total']) * 100\n",
    "    else:\n",
    "        stats['fidelity_rate'] = None\n",
    "    return stats\n",
    "\n",
    "# Test loading\n",
    "test = load_layer1_jsonl('gpt-5', 'baseline_en')\n",
    "if test:\n",
    "    print(\"Sample Layer 1 stats (from JSONL):\")\n",
    "    print(f\"  total: {test['total']}, match: {test['match']}, mismatch: {test['mismatch']}\")\n",
    "    print(f\"  fidelity_rate: {test['fidelity_rate']:.1f}%\")\n",
    "else:\n",
    "    print(\"No Layer 1 data found for gpt-5 baseline_en\")\n",
    "    print(\"Available files:\", glob.glob('results/layer1/gpt-5/*.jsonl')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Layer 2 (Task Accuracy) from Evaluated JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:02.493239Z",
     "iopub.status.busy": "2026-02-05T02:35:02.493128Z",
     "iopub.status.idle": "2026-02-05T02:35:02.505909Z",
     "shell.execute_reply": "2026-02-05T02:35:02.505541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Layer 2 stats:\n",
      "  {'total': 182, 'passed': 104, 'failed': 78, 'error': 0, 'parse_error': 0}\n"
     ]
    }
   ],
   "source": [
    "def load_layer2_jsonl(model, condition):\n",
    "    \"\"\"Load Layer 2 evaluated JSONL and compute stats.\"\"\"\n",
    "    patterns = [\n",
    "        f'../results/layer2/{model}/evaluated_{condition}_*.jsonl',\n",
    "        f'../results/layer2/{model}/evaluated_{condition}.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        # Exclude variance runs (run2, run3) - only use primary results\n",
    "        files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n",
    "        if files:\n",
    "            stats = {'total': 0, 'passed': 0, 'failed': 0, 'error': 0, 'parse_error': 0}\n",
    "            with open(files[-1]) as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        stats['total'] += 1\n",
    "                        eval_result = data.get('evaluation', {})\n",
    "                        if eval_result.get('passed'):\n",
    "                            stats['passed'] += 1\n",
    "                        else:\n",
    "                            stats['failed'] += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        stats['parse_error'] += 1\n",
    "            return stats\n",
    "    return None\n",
    "\n",
    "# Test loading\n",
    "test = load_layer2_jsonl('gpt-5', 'baseline_en')\n",
    "if test:\n",
    "    print(\"Sample Layer 2 stats:\")\n",
    "    print(f\"  {test}\")\n",
    "else:\n",
    "    print(\"No Layer 2 data found\")\n",
    "    print(\"Available files:\", glob.glob('results/layer2/gpt-5/*.jsonl')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Response Files for Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:02.508259Z",
     "iopub.status.busy": "2026-02-05T02:35:02.508046Z",
     "iopub.status.idle": "2026-02-05T02:35:02.548562Z",
     "shell.execute_reply": "2026-02-05T02:35:02.518910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample response stats:\n",
      "  {'total': 182, 'success': 182, 'api_error': 0, 'empty_response': 0, 'parse_error': 0}\n"
     ]
    }
   ],
   "source": [
    "def check_response_errors(model, condition):\n",
    "    \"\"\"Check response files for errors.\"\"\"\n",
    "    patterns = [\n",
    "        f'../results/responses/{model}/responses_{condition}_*.jsonl',\n",
    "        f'../results/responses/{model}/responses_{condition}.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        # Exclude variance runs (run2, run3) - only use primary results\n",
    "        files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n",
    "        if files:\n",
    "            stats = {\n",
    "                'total': 0,\n",
    "                'success': 0,\n",
    "                'api_error': 0,\n",
    "                'empty_response': 0,\n",
    "                'parse_error': 0\n",
    "            }\n",
    "            with open(files[-1]) as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        stats['total'] += 1\n",
    "                        if data.get('success'):\n",
    "                            stats['success'] += 1\n",
    "                            if not data.get('response') or data.get('response', '').strip() == '':\n",
    "                                stats['empty_response'] += 1\n",
    "                        else:\n",
    "                            stats['api_error'] += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        stats['parse_error'] += 1\n",
    "            return stats\n",
    "    return None\n",
    "\n",
    "# Test\n",
    "test = check_response_errors('gpt-5', 'baseline_en')\n",
    "if test:\n",
    "    print(\"Sample response stats:\")\n",
    "    print(f\"  {test}\")\n",
    "else:\n",
    "    print(\"No response files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Language Fidelity Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:02.628972Z",
     "iopub.status.busy": "2026-02-05T02:35:02.616722Z",
     "iopub.status.idle": "2026-02-05T02:35:02.716226Z",
     "shell.execute_reply": "2026-02-05T02:35:02.715871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUAGE FIDELITY (%)\n",
      "========================================================================================================================\n",
      "                 baseline_en  baseline_de  baseline_zh  baseline_es  baseline_ar  en_to_de  en_to_zh  en_to_es  en_to_ar  de_to_en  zh_to_en  es_to_en  ar_to_en\n",
      "Model                                                                                                                                                           \n",
      "GPT-5                  100.0         98.9        100.0        100.0         99.5      97.8      99.5      99.5      97.8      94.0      95.6      94.5      96.2\n",
      "Gemini 3 Pro           100.0         98.9        100.0        100.0         99.5      98.3      98.9      98.4      97.8      78.6      72.5      74.7      69.2\n",
      "Claude Opus 4.5        100.0         98.9        100.0        100.0         99.5      96.7      94.0      97.3      96.7      10.4       9.9       6.0       4.4\n",
      "DeepSeek-V3.1          100.0         98.9         98.4        100.0         98.9      93.4      73.1      95.1      91.8      41.8      60.4      41.2      64.3\n",
      "Command R+             100.0         98.9        100.0        100.0         99.5      91.8      89.0      95.6      80.8       1.1       1.1       0.5       0.5\n"
     ]
    }
   ],
   "source": [
    "fidelity_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for cond in CONDITIONS:\n",
    "        stats = load_layer1_jsonl(model_id, cond)\n",
    "        if stats:\n",
    "            row[cond] = stats['fidelity_rate']\n",
    "        else:\n",
    "            row[cond] = None\n",
    "    fidelity_data.append(row)\n",
    "\n",
    "df_fidelity = pd.DataFrame(fidelity_data).set_index('Model')\n",
    "print(\"LANGUAGE FIDELITY (%)\")\n",
    "print(\"=\" * 120)\n",
    "print(df_fidelity.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Task Accuracy Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:02.717682Z",
     "iopub.status.busy": "2026-02-05T02:35:02.717585Z",
     "iopub.status.idle": "2026-02-05T02:35:03.406785Z",
     "shell.execute_reply": "2026-02-05T02:35:03.406258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK ACCURACY (%)\n",
      "========================================================================================================================\n",
      "                 baseline_en  baseline_de  baseline_zh  baseline_es  baseline_ar  en_to_de  en_to_zh  en_to_es  en_to_ar  de_to_en  zh_to_en  es_to_en  ar_to_en\n",
      "Model                                                                                                                                                           \n",
      "GPT-5                   57.1         58.2         57.7         57.7         61.0      57.1      59.9      59.3      60.4      55.5      50.5      49.5      54.4\n",
      "Gemini 3 Pro            71.4         66.5         72.0         71.4         70.3      73.6      70.3      68.7      70.9      66.5      68.7      72.0      74.2\n",
      "Claude Opus 4.5         54.4         45.1         48.9         52.7         47.3      49.5      46.7      50.5      48.9      48.4      47.8      52.7      50.5\n",
      "DeepSeek-V3.1           50.0         39.0         39.0         45.1         37.9      40.1      44.5      44.0      42.9      38.5      37.4      37.9      36.8\n",
      "Command R+              15.9         11.5          9.3          9.9         14.3      15.4      13.2      15.4      15.9      12.1      11.0      11.5      11.0\n"
     ]
    }
   ],
   "source": [
    "accuracy_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for cond in CONDITIONS:\n",
    "        stats = load_layer2_jsonl(model_id, cond)\n",
    "        if stats and stats['total'] > 0:\n",
    "            row[cond] = (stats['passed'] / stats['total']) * 100\n",
    "        else:\n",
    "            row[cond] = None\n",
    "    accuracy_data.append(row)\n",
    "\n",
    "df_accuracy = pd.DataFrame(accuracy_data).set_index('Model')\n",
    "print(\"TASK ACCURACY (%)\")\n",
    "print(\"=\" * 120)\n",
    "print(df_accuracy.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Detection Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:03.410463Z",
     "iopub.status.busy": "2026-02-05T02:35:03.410162Z",
     "iopub.status.idle": "2026-02-05T02:35:03.928478Z",
     "shell.execute_reply": "2026-02-05T02:35:03.927973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR DETECTION REPORT\n",
      "================================================================================\n",
      "\n",
      "GPT-5:\n",
      "  No errors found\n",
      "\n",
      "Gemini 3 Pro:\n",
      "  baseline_en: Empty: 2\n",
      "  baseline_de: Empty: 1\n",
      "  baseline_es: Empty: 1\n",
      "  en_to_de: Empty: 2\n",
      "  en_to_zh: Empty: 1\n",
      "  en_to_ar: Empty: 1\n",
      "\n",
      "Claude Opus 4.5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  baseline_de: Empty: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  baseline_es: Empty: 1\n",
      "  en_to_de: Empty: 1\n",
      "\n",
      "DeepSeek-V3.1:\n",
      "  No errors found\n",
      "\n",
      "Command R+:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No errors found\n"
     ]
    }
   ],
   "source": [
    "print(\"ERROR DETECTION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    has_errors = False\n",
    "    for cond in CONDITIONS:\n",
    "        stats = check_response_errors(model_id, cond)\n",
    "        if stats:\n",
    "            errors = []\n",
    "            if stats['api_error'] > 0:\n",
    "                errors.append(f\"API errors: {stats['api_error']}\")\n",
    "            if stats['empty_response'] > 0:\n",
    "                errors.append(f\"Empty: {stats['empty_response']}\")\n",
    "            if stats['parse_error'] > 0:\n",
    "                errors.append(f\"Parse errors: {stats['parse_error']}\")\n",
    "            if errors:\n",
    "                print(f\"  {cond}: {', '.join(errors)}\")\n",
    "                has_errors = True\n",
    "    if not has_errors:\n",
    "        print(\"  No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conversation Length Effect (Xâ†’EN Fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.010678Z",
     "iopub.status.busy": "2026-02-05T02:35:04.009392Z",
     "iopub.status.idle": "2026-02-05T02:35:04.466705Z",
     "shell.execute_reply": "2026-02-05T02:35:04.466328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn Distribution in Dataset:\n",
      "==================================================\n",
      "   3 turns:   7 (Short)\n",
      "   5 turns:  37 (Short)\n",
      "   7 turns:  68 (Medium)\n",
      "   9 turns:  31 (Medium)\n",
      "  11 turns:  18 (Long)\n",
      "  13 turns:  11 (Long)\n",
      "  15 turns:   8 (Long)\n",
      "  19 turns:   2 (Long)\n",
      "\n",
      "New categories:\n",
      "  Short (3-5 turns):   n=44\n",
      "  Medium (7-9 turns):  n=99\n",
      "  Long (11+ turns):    n=39\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# New turn ranges: Short (3-5), Medium (7-9), Long (11+)\n",
    "def categorize_length_new(turns):\n",
    "    \"\"\"Categorize conversation length with new ranges.\"\"\"\n",
    "    if 3 <= turns <= 5:\n",
    "        return 'Short'\n",
    "    elif 7 <= turns <= 9:\n",
    "        return 'Medium'\n",
    "    elif turns >= 11:\n",
    "        return 'Long'\n",
    "    else:\n",
    "        return None  # Exclude 2, 6, 10 turns\n",
    "\n",
    "# Load baseline data with turn counts\n",
    "baseline_data = {}\n",
    "with open('../data/experiments/baseline_en.jsonl') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line.strip())\n",
    "        baseline_data[item['QUESTION_ID']] = {\n",
    "            'turns': len(item['CONVERSATION'])\n",
    "        }\n",
    "\n",
    "# Check turn distribution\n",
    "from collections import Counter\n",
    "turn_counts = [v['turns'] for v in baseline_data.values()]\n",
    "print(\"Turn Distribution in Dataset:\")\n",
    "print(\"=\" * 50)\n",
    "for t, count in sorted(Counter(turn_counts).items()):\n",
    "    cat = categorize_length_new(t)\n",
    "    cat_str = f\"({cat})\" if cat else \"(excluded)\"\n",
    "    print(f\"  {t:2d} turns: {count:3d} {cat_str}\")\n",
    "\n",
    "# Count by new categories\n",
    "short = sum(1 for t in turn_counts if 3 <= t <= 5)\n",
    "medium = sum(1 for t in turn_counts if 7 <= t <= 9)\n",
    "long = sum(1 for t in turn_counts if t >= 11)\n",
    "print(f\"\\nNew categories:\")\n",
    "print(f\"  Short (3-5 turns):   n={short}\")\n",
    "print(f\"  Medium (7-9 turns):  n={medium}\")\n",
    "print(f\"  Long (11+ turns):    n={long}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.468327Z",
     "iopub.status.busy": "2026-02-05T02:35:04.468206Z",
     "iopub.status.idle": "2026-02-05T02:35:04.611327Z",
     "shell.execute_reply": "2026-02-05T02:35:04.610909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X->EN Fidelity by Conversation Length (New Ranges)\n",
      "================================================================================\n",
      "Model                    Short (3-5)       Med (7-9)      Long (11+)      p-value\n",
      "--------------------------------------------------------------------------------\n",
      "GPT-5                          97.2%           93.9%           95.5%         0.25\n",
      "Gemini 3 Pro                   82.4%           75.0%           60.9%       <0.001\n",
      "Claude Opus 4.5                11.9%            5.8%            7.7%         0.04\n",
      "DeepSeek-V3.1                  55.1%           54.0%           42.9%         0.04\n",
      "Command R+                      0.0%            0.5%            2.6%         0.02\n"
     ]
    }
   ],
   "source": [
    "def load_responses_with_turns(model, condition):\n",
    "    \"\"\"Load responses and extract turn counts.\"\"\"\n",
    "    patterns = [\n",
    "        f'../results/responses/{model}/responses_{condition}_*.jsonl',\n",
    "        f'../results/responses/{model}/responses_{condition}.jsonl',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        # Exclude variance runs (run2, run3) - only use primary results\n",
    "        files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n",
    "        if files:\n",
    "            data = []\n",
    "            with open(files[-1]) as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line.strip())\n",
    "                    if item.get('success'):\n",
    "                        data.append({\n",
    "                            'question_id': item.get('question_id'),\n",
    "                            'turn_count': item.get('turn_count'),\n",
    "                        })\n",
    "            return data\n",
    "    return None\n",
    "\n",
    "def load_language_eval_by_qid(model, condition):\n",
    "    \"\"\"Load language evaluation results by question ID.\"\"\"\n",
    "    pattern = f'../results/layer1/{model}/language_eval_{condition}_*.jsonl'\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    # Exclude variance runs (run2, run3) - only use primary results\n",
    "    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    \n",
    "    data = {}\n",
    "    with open(files[-1]) as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "            qid = item.get('question_id')\n",
    "            data[qid] = item.get('match_status') == 'match'\n",
    "    return data\n",
    "\n",
    "def compute_fidelity_by_length_new(model):\n",
    "    \"\"\"Compute X->EN fidelity by conversation length (new ranges).\"\"\"\n",
    "    results = {'Short': {'match': 0, 'total': 0},\n",
    "               'Medium': {'match': 0, 'total': 0},\n",
    "               'Long': {'match': 0, 'total': 0}}\n",
    "    \n",
    "    for lang in LANGS:\n",
    "        condition = f\"{lang}_to_en\"\n",
    "        \n",
    "        responses = load_responses_with_turns(model, condition)\n",
    "        lang_eval = load_language_eval_by_qid(model, condition)\n",
    "        \n",
    "        if not responses or not lang_eval:\n",
    "            continue\n",
    "        \n",
    "        for resp in responses:\n",
    "            qid = resp['question_id']\n",
    "            turns = resp['turn_count']\n",
    "            category = categorize_length_new(turns)\n",
    "            \n",
    "            if category and qid in lang_eval:\n",
    "                results[category]['total'] += 1\n",
    "                if lang_eval[qid]:\n",
    "                    results[category]['match'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "def chi_square_test(results):\n",
    "    \"\"\"Run chi-square test for independence between length and fidelity.\"\"\"\n",
    "    observed = []\n",
    "    for cat in ['Short', 'Medium', 'Long']:\n",
    "        match = results[cat]['match']\n",
    "        total = results[cat]['total']\n",
    "        if total == 0:\n",
    "            return None, None\n",
    "        observed.append([match, total - match])\n",
    "    \n",
    "    observed = np.array(observed).T\n",
    "    \n",
    "    # Check if test is valid\n",
    "    if np.any(observed.sum(axis=0) < 5):\n",
    "        return None, \"Low counts\"\n",
    "    \n",
    "    try:\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(observed)\n",
    "        return chi2, p\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "# Compute for all models\n",
    "length_results = {}\n",
    "for model_id in MODELS.keys():\n",
    "    length_results[model_id] = compute_fidelity_by_length_new(model_id)\n",
    "\n",
    "print(\"X->EN Fidelity by Conversation Length (New Ranges)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<20} {'Short (3-5)':>15} {'Med (7-9)':>15} {'Long (11+)':>15} {'p-value':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    res = length_results[model_id]\n",
    "    chi2, p = chi_square_test(res)\n",
    "    \n",
    "    row = []\n",
    "    for cat in ['Short', 'Medium', 'Long']:\n",
    "        if res[cat]['total'] > 0:\n",
    "            pct = res[cat]['match'] / res[cat]['total'] * 100\n",
    "            row.append(f\"{pct:.1f}%\")\n",
    "        else:\n",
    "            row.append(\"--\")\n",
    "    \n",
    "    if p is not None and isinstance(p, float):\n",
    "        p_str = \"<0.001\" if p < 0.001 else f\"{p:.2f}\"\n",
    "    else:\n",
    "        p_str = \"---\"\n",
    "    \n",
    "    print(f\"{model_name:<20} {row[0]:>15} {row[1]:>15} {row[2]:>15} {p_str:>12}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
