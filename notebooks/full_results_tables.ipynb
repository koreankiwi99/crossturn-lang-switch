{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Results Tables\n",
    "Generate tables for Language Fidelity and Task Accuracy with error detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "MODELS = {\n",
    "    'gpt-5': 'GPT-5',\n",
    "    'gemini-3-pro': 'Gemini 3 Pro',\n",
    "    'claude-opus-4.5': 'Claude Opus 4.5',\n",
    "    'deepseek-v3.1': 'DeepSeek-V3.1',\n",
    "    'command-r-plus': 'Command R+'\n",
    "}\n",
    "\n",
    "LANGS = ['de', 'zh', 'es', 'ar']\n",
    "CONDITIONS = ['baseline_en'] + [f'baseline_{l}' for l in LANGS] + [f'en_to_{l}' for l in LANGS] + [f'{l}_to_en' for l in LANGS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Layer 1 (Language Fidelity) from Summary JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer1_json(model, condition):\n",
    "    \"\"\"Load Layer 1 summary JSON file.\"\"\"\n",
    "    pattern = f'results/layer1/{model}/language_summary_{condition}_*.json'\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        return None\n",
    "    with open(files[-1]) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Test loading\n",
    "test = load_layer1_json('gpt-5', 'baseline_en')\n",
    "if test:\n",
    "    print(\"Sample Layer 1 JSON:\")\n",
    "    print(json.dumps(test, indent=2))\n",
    "else:\n",
    "    print(\"No Layer 1 data found for gpt-5 baseline_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Layer 2 (Task Accuracy) from Evaluated JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer2_jsonl(model, condition):\n",
    "    \"\"\"Load Layer 2 evaluated JSONL and compute stats.\"\"\"\n",
    "    patterns = [\n",
    "        f'results/layer2/{model}/evaluated_{condition}_*.jsonl',\n",
    "        f'results/layer2/{model}/evaluated_{condition}.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        if files:\n",
    "            stats = {'total': 0, 'passed': 0, 'failed': 0, 'error': 0, 'parse_error': 0}\n",
    "            with open(files[-1]) as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        stats['total'] += 1\n",
    "                        eval_result = data.get('evaluation', {})\n",
    "                        if eval_result.get('passed'):\n",
    "                            stats['passed'] += 1\n",
    "                        else:\n",
    "                            stats['failed'] += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        stats['parse_error'] += 1\n",
    "            return stats\n",
    "    return None\n",
    "\n",
    "# Test loading\n",
    "test = load_layer2_jsonl('gpt-5', 'baseline_en')\n",
    "if test:\n",
    "    print(\"Sample Layer 2 stats:\")\n",
    "    print(json.dumps(test, indent=2))\n",
    "else:\n",
    "    print(\"No Layer 2 data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Response Files for Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_response_errors(model, condition):\n",
    "    \"\"\"Check response files for errors.\"\"\"\n",
    "    patterns = [\n",
    "        f'results/responses/{model}/responses_{condition}_*.jsonl',\n",
    "        f'results/responses/{model}/responses_{condition}.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        if files:\n",
    "            stats = {\n",
    "                'total': 0,\n",
    "                'success': 0,\n",
    "                'api_error': 0,\n",
    "                'empty_response': 0,\n",
    "                'parse_error': 0\n",
    "            }\n",
    "            with open(files[-1]) as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        stats['total'] += 1\n",
    "                        if data.get('success'):\n",
    "                            stats['success'] += 1\n",
    "                            if not data.get('response') or data.get('response', '').strip() == '':\n",
    "                                stats['empty_response'] += 1\n",
    "                        else:\n",
    "                            stats['api_error'] += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        stats['parse_error'] += 1\n",
    "            return stats\n",
    "    return None\n",
    "\n",
    "# Test\n",
    "test = check_response_errors('gpt-5', 'baseline_en')\n",
    "if test:\n",
    "    print(\"Sample response stats:\")\n",
    "    print(json.dumps(test, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Language Fidelity Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidelity_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for cond in CONDITIONS:\n",
    "        data = load_layer1_json(model_id, cond)\n",
    "        if data:\n",
    "            row[cond] = data.get('fidelity_rate')\n",
    "        else:\n",
    "            row[cond] = None\n",
    "    fidelity_data.append(row)\n",
    "\n",
    "df_fidelity = pd.DataFrame(fidelity_data).set_index('Model')\n",
    "print(\"LANGUAGE FIDELITY (%)\")\n",
    "print(\"=\" * 100)\n",
    "print(df_fidelity.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Task Accuracy Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for cond in CONDITIONS:\n",
    "        stats = load_layer2_jsonl(model_id, cond)\n",
    "        if stats and stats['total'] > 0:\n",
    "            row[cond] = (stats['passed'] / stats['total']) * 100\n",
    "        else:\n",
    "            row[cond] = None\n",
    "    accuracy_data.append(row)\n",
    "\n",
    "df_accuracy = pd.DataFrame(accuracy_data).set_index('Model')\n",
    "print(\"TASK ACCURACY (%)\")\n",
    "print(\"=\" * 100)\n",
    "print(df_accuracy.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Detection Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ERROR DETECTION REPORT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    has_errors = False\n",
    "    for cond in CONDITIONS:\n",
    "        stats = check_response_errors(model_id, cond)\n",
    "        if stats:\n",
    "            errors = []\n",
    "            if stats['api_error'] > 0:\n",
    "                errors.append(f\"API errors: {stats['api_error']}\")\n",
    "            if stats['empty_response'] > 0:\n",
    "                errors.append(f\"Empty: {stats['empty_response']}\")\n",
    "            if stats['parse_error'] > 0:\n",
    "                errors.append(f\"Parse errors: {stats['parse_error']}\")\n",
    "            if errors:\n",
    "                print(f\"  {cond}: {', '.join(errors)}\")\n",
    "                has_errors = True\n",
    "    if not has_errors:\n",
    "        print(\"  No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LaTeX Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt(val):\n",
    "    if val is None or pd.isna(val):\n",
    "        return '--'\n",
    "    return f'{val:.1f}'\n",
    "\n",
    "print(\"LaTeX: LANGUAGE FIDELITY\")\n",
    "print(\"Model & Base EN & Base DE & Base ZH & Base ES & Base AR & EN->DE & EN->ZH & EN->ES & EN->AR & DE->EN & ZH->EN & ES->EN & AR->EN\")\n",
    "for _, row in df_fidelity.iterrows():\n",
    "    vals = [fmt(row.get(c)) for c in CONDITIONS]\n",
    "    print(f\"{row.name} & {' & '.join(vals)} \\\\\\\\\")\n",
    "\n",
    "print(\"\\nLaTeX: TASK ACCURACY\")\n",
    "for _, row in df_accuracy.iterrows():\n",
    "    vals = [fmt(row.get(c)) for c in CONDITIONS]\n",
    "    print(f\"{row.name} & {' & '.join(vals)} \\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
