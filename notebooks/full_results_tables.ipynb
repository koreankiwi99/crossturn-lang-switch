{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Results Tables\n",
    "Generate tables for Language Fidelity and Task Accuracy with error detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer1 exists: True\n",
      "Layer2 exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to project root\n",
    "#os.chdir(Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent)\n",
    "#print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "MODELS = {\n",
    "    'gpt-5': 'GPT-5',\n",
    "    'gemini-3-pro': 'Gemini 3 Pro',\n",
    "    'claude-opus-4.5': 'Claude Opus 4.5',\n",
    "    'deepseek-v3.1': 'DeepSeek-V3.1',\n",
    "    'command-r-plus': 'Command R+'\n",
    "}\n",
    "\n",
    "LANGS = ['de', 'zh', 'es', 'ar']\n",
    "CONDITIONS = ['baseline_en'] + [f'baseline_{l}' for l in LANGS] + [f'en_to_{l}' for l in LANGS] + [f'{l}_to_en' for l in LANGS]\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Layer1 exists: {os.path.exists('../results/layer1')}\")\n",
    "print(f\"Layer2 exists: {os.path.exists('../results/layer2')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Layer 1 (Language Fidelity) from JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Layer 1 stats (from JSONL):\n",
      "  total: 182, match: 182, mismatch: 0\n",
      "  fidelity_rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def load_layer1_jsonl(model, condition):\n",
    "    \"\"\"Load Layer 1 evaluation JSONL and compute stats.\"\"\"\n",
    "    pattern = f'../results/layer1/{model}/language_eval_{condition}_*.jsonl'\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    stats = {'total': 0, 'match': 0, 'mismatch': 0, 'mixed': 0, 'error': 0, 'parse_error': 0}\n",
    "    with open(files[-1]) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                stats['total'] += 1\n",
    "                status = data.get('match_status', '')\n",
    "                if status == 'match':\n",
    "                    stats['match'] += 1\n",
    "                elif status == 'mismatch':\n",
    "                    stats['mismatch'] += 1\n",
    "                elif status == 'mixed':\n",
    "                    stats['mixed'] += 1\n",
    "                else:\n",
    "                    stats['error'] += 1\n",
    "            except json.JSONDecodeError:\n",
    "                stats['parse_error'] += 1\n",
    "\n",
    "    if stats['total'] > 0:\n",
    "        stats['fidelity_rate'] = (stats['match'] / stats['total']) * 100\n",
    "    else:\n",
    "        stats['fidelity_rate'] = None\n",
    "    return stats\n",
    "\n",
    "# Test loading\n",
    "test = load_layer1_jsonl('gpt-5', 'baseline_en')\n",
    "if test:\n",
    "    print(\"Sample Layer 1 stats (from JSONL):\")\n",
    "    print(f\"  total: {test['total']}, match: {test['match']}, mismatch: {test['mismatch']}\")\n",
    "    print(f\"  fidelity_rate: {test['fidelity_rate']:.1f}%\")\n",
    "else:\n",
    "    print(\"No Layer 1 data found for gpt-5 baseline_en\")\n",
    "    print(\"Available files:\", glob.glob('results/layer1/gpt-5/*.jsonl')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Layer 2 (Task Accuracy) from Evaluated JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Layer 2 stats:\n",
      "  {'total': 182, 'passed': 104, 'failed': 78, 'error': 0, 'parse_error': 0}\n"
     ]
    }
   ],
   "source": [
    "def load_layer2_jsonl(model, condition):\n",
    "    \"\"\"Load Layer 2 evaluated JSONL and compute stats.\"\"\"\n",
    "    patterns = [\n",
    "        f'../results/layer2/{model}/evaluated_{condition}_*.jsonl',\n",
    "        f'../results/layer2/{model}/evaluated_{condition}.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        if files:\n",
    "            stats = {'total': 0, 'passed': 0, 'failed': 0, 'error': 0, 'parse_error': 0}\n",
    "            with open(files[-1]) as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        stats['total'] += 1\n",
    "                        eval_result = data.get('evaluation', {})\n",
    "                        if eval_result.get('passed'):\n",
    "                            stats['passed'] += 1\n",
    "                        else:\n",
    "                            stats['failed'] += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        stats['parse_error'] += 1\n",
    "            return stats\n",
    "    return None\n",
    "\n",
    "# Test loading\n",
    "test = load_layer2_jsonl('gpt-5', 'baseline_en')\n",
    "if test:\n",
    "    print(\"Sample Layer 2 stats:\")\n",
    "    print(f\"  {test}\")\n",
    "else:\n",
    "    print(\"No Layer 2 data found\")\n",
    "    print(\"Available files:\", glob.glob('results/layer2/gpt-5/*.jsonl')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Response Files for Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample response stats:\n",
      "  {'total': 182, 'success': 182, 'api_error': 0, 'empty_response': 0, 'parse_error': 0}\n"
     ]
    }
   ],
   "source": [
    "def check_response_errors(model, condition):\n",
    "    \"\"\"Check response files for errors.\"\"\"\n",
    "    patterns = [\n",
    "        f'../results/responses/{model}/responses_{condition}_*.jsonl',\n",
    "        f'../results/responses/{model}/responses_{condition}.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        if files:\n",
    "            stats = {\n",
    "                'total': 0,\n",
    "                'success': 0,\n",
    "                'api_error': 0,\n",
    "                'empty_response': 0,\n",
    "                'parse_error': 0\n",
    "            }\n",
    "            with open(files[-1]) as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        stats['total'] += 1\n",
    "                        if data.get('success'):\n",
    "                            stats['success'] += 1\n",
    "                            if not data.get('response') or data.get('response', '').strip() == '':\n",
    "                                stats['empty_response'] += 1\n",
    "                        else:\n",
    "                            stats['api_error'] += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        stats['parse_error'] += 1\n",
    "            return stats\n",
    "    return None\n",
    "\n",
    "# Test\n",
    "test = check_response_errors('gpt-5', 'baseline_en')\n",
    "if test:\n",
    "    print(\"Sample response stats:\")\n",
    "    print(f\"  {test}\")\n",
    "else:\n",
    "    print(\"No response files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Language Fidelity Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUAGE FIDELITY (%)\n",
      "========================================================================================================================\n",
      "                 baseline_en  baseline_de  baseline_zh  baseline_es  baseline_ar  en_to_de  en_to_zh  en_to_es  en_to_ar  de_to_en  zh_to_en  es_to_en  ar_to_en\n",
      "Model                                                                                                                                                           \n",
      "GPT-5                  100.0         98.9        100.0        100.0         99.5      97.8      99.5      99.5      97.8      94.0      95.6      94.5      96.2\n",
      "Gemini 3 Pro           100.0         98.9        100.0        100.0         99.5      98.3      98.9      98.4      97.8      78.6      72.5      74.7      69.2\n",
      "Claude Opus 4.5        100.0         98.9        100.0        100.0         99.5      96.7      94.0      97.3      96.7      10.4       9.9       6.0       4.4\n",
      "DeepSeek-V3.1          100.0         98.9         98.4        100.0         98.9      93.4      73.1      95.1      91.8      41.8      60.4      41.2      64.3\n",
      "Command R+               NaN         98.9        100.0        100.0         99.5      91.8      89.0      95.6      80.8       1.1       1.1       0.5       0.5\n"
     ]
    }
   ],
   "source": [
    "fidelity_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for cond in CONDITIONS:\n",
    "        stats = load_layer1_jsonl(model_id, cond)\n",
    "        if stats:\n",
    "            row[cond] = stats['fidelity_rate']\n",
    "        else:\n",
    "            row[cond] = None\n",
    "    fidelity_data.append(row)\n",
    "\n",
    "df_fidelity = pd.DataFrame(fidelity_data).set_index('Model')\n",
    "print(\"LANGUAGE FIDELITY (%)\")\n",
    "print(\"=\" * 120)\n",
    "print(df_fidelity.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Task Accuracy Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK ACCURACY (%)\n",
      "========================================================================================================================\n",
      "                 baseline_en  baseline_de  baseline_zh  baseline_es  baseline_ar  en_to_de  en_to_zh  en_to_es  en_to_ar  de_to_en  zh_to_en  es_to_en  ar_to_en\n",
      "Model                                                                                                                                                           \n",
      "GPT-5                   57.1         58.2         57.7         57.7         61.0      57.1      59.9      59.3      60.4      55.5      50.5      49.5      54.4\n",
      "Gemini 3 Pro            71.4         66.5         72.0         71.4         70.3      73.6      70.3      68.7      70.9      66.5      68.7      72.0      74.2\n",
      "Claude Opus 4.5         54.4         45.1         48.9         52.7         47.3      49.5      46.7      50.5      48.9      48.4      47.8      52.7      50.5\n",
      "DeepSeek-V3.1           50.0         39.0         39.0         45.1         37.9      40.1      44.5      44.0      42.9      38.5      37.4      37.9      36.8\n",
      "Command R+               NaN         11.5          9.3          9.9         14.3      15.4      13.2      15.4      15.9      12.1      11.0      11.5      11.0\n"
     ]
    }
   ],
   "source": [
    "accuracy_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for cond in CONDITIONS:\n",
    "        stats = load_layer2_jsonl(model_id, cond)\n",
    "        if stats and stats['total'] > 0:\n",
    "            row[cond] = (stats['passed'] / stats['total']) * 100\n",
    "        else:\n",
    "            row[cond] = None\n",
    "    accuracy_data.append(row)\n",
    "\n",
    "df_accuracy = pd.DataFrame(accuracy_data).set_index('Model')\n",
    "print(\"TASK ACCURACY (%)\")\n",
    "print(\"=\" * 120)\n",
    "print(df_accuracy.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Detection Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR DETECTION REPORT\n",
      "================================================================================\n",
      "\n",
      "GPT-5:\n",
      "  No errors found\n",
      "\n",
      "Gemini 3 Pro:\n",
      "  baseline_en: Empty: 2\n",
      "  baseline_de: Empty: 1\n",
      "  baseline_es: Empty: 1\n",
      "  en_to_de: Empty: 2\n",
      "  en_to_zh: Empty: 1\n",
      "  en_to_ar: Empty: 1\n",
      "\n",
      "Claude Opus 4.5:\n",
      "  baseline_de: Empty: 1\n",
      "  baseline_es: Empty: 1\n",
      "  en_to_de: Empty: 1\n",
      "\n",
      "DeepSeek-V3.1:\n",
      "  No errors found\n",
      "\n",
      "Command R+:\n",
      "  No errors found\n"
     ]
    }
   ],
   "source": [
    "print(\"ERROR DETECTION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    has_errors = False\n",
    "    for cond in CONDITIONS:\n",
    "        stats = check_response_errors(model_id, cond)\n",
    "        if stats:\n",
    "            errors = []\n",
    "            if stats['api_error'] > 0:\n",
    "                errors.append(f\"API errors: {stats['api_error']}\")\n",
    "            if stats['empty_response'] > 0:\n",
    "                errors.append(f\"Empty: {stats['empty_response']}\")\n",
    "            if stats['parse_error'] > 0:\n",
    "                errors.append(f\"Parse errors: {stats['parse_error']}\")\n",
    "            if errors:\n",
    "                print(f\"  {cond}: {', '.join(errors)}\")\n",
    "                has_errors = True\n",
    "    if not has_errors:\n",
    "        print(\"  No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LaTeX Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX: LANGUAGE FIDELITY\n",
      "================================================================================\n",
      "GPT-5 & 100.0 & 98.9 & 100.0 & 100.0 & 99.5 & 97.8 & 99.5 & 99.5 & 97.8 & 94.0 & 95.6 & 94.5 & 96.2 \\\\\n",
      "Gemini 3 Pro & 100.0 & 98.9 & 100.0 & 100.0 & 99.5 & 98.3 & 98.9 & 98.4 & 97.8 & 78.6 & 72.5 & 74.7 & 69.2 \\\\\n",
      "Claude Opus 4.5 & 100.0 & 98.9 & 100.0 & 100.0 & 99.5 & 96.7 & 94.0 & 97.3 & 96.7 & 10.4 & 9.9 & 6.0 & 4.4 \\\\\n",
      "DeepSeek-V3.1 & 100.0 & 98.9 & 98.4 & 100.0 & 98.9 & 93.4 & 73.1 & 95.1 & 91.8 & 41.8 & 60.4 & 41.2 & 64.3 \\\\\n",
      "Command R+ & -- & 98.9 & 100.0 & 100.0 & 99.5 & 91.8 & 89.0 & 95.6 & 80.8 & 1.1 & 1.1 & 0.5 & 0.5 \\\\\n",
      "\n",
      "LaTeX: TASK ACCURACY\n",
      "================================================================================\n",
      "GPT-5 & 57.1 & 58.2 & 57.7 & 57.7 & 61.0 & 57.1 & 59.9 & 59.3 & 60.4 & 55.5 & 50.5 & 49.5 & 54.4 \\\\\n",
      "Gemini 3 Pro & 71.4 & 66.5 & 72.0 & 71.4 & 70.3 & 73.6 & 70.3 & 68.7 & 70.9 & 66.5 & 68.7 & 72.0 & 74.2 \\\\\n",
      "Claude Opus 4.5 & 54.4 & 45.1 & 48.9 & 52.7 & 47.3 & 49.5 & 46.7 & 50.5 & 48.9 & 48.4 & 47.8 & 52.7 & 50.5 \\\\\n",
      "DeepSeek-V3.1 & 50.0 & 39.0 & 39.0 & 45.1 & 37.9 & 40.1 & 44.5 & 44.0 & 42.9 & 38.5 & 37.4 & 37.9 & 36.8 \\\\\n",
      "Command R+ & -- & 11.5 & 9.3 & 9.9 & 14.3 & 15.4 & 13.2 & 15.4 & 15.9 & 12.1 & 11.0 & 11.5 & 11.0 \\\\\n"
     ]
    }
   ],
   "source": [
    "def fmt(val):\n",
    "    if val is None or pd.isna(val):\n",
    "        return '--'\n",
    "    return f'{val:.1f}'\n",
    "\n",
    "print(\"LaTeX: LANGUAGE FIDELITY\")\n",
    "print(\"=\"*80)\n",
    "for _, row in df_fidelity.iterrows():\n",
    "    vals = [fmt(row.get(c)) for c in CONDITIONS]\n",
    "    print(f\"{row.name} & {' & '.join(vals)} \\\\\\\\\")\n",
    "\n",
    "print(\"\\nLaTeX: TASK ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "for _, row in df_accuracy.iterrows():\n",
    "    vals = [fmt(row.get(c)) for c in CONDITIONS]\n",
    "    print(f\"{row.name} & {' & '.join(vals)} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Main Results Summary (Averaged Across Languages)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Calculate average fidelity across languages for EN→X and X→EN conditions\nprint(\"MAIN RESULTS: Language Fidelity by Condition (Averaged Across Languages)\")\nprint(\"=\" * 80)\n\nen_to_x_cols = [f'en_to_{l}' for l in LANGS]\nx_to_en_cols = [f'{l}_to_en' for l in LANGS]\n\nsummary_data = []\nfor model_id, model_name in MODELS.items():\n    row = df_fidelity.loc[model_name]\n    \n    # EN→X average\n    en_to_x_vals = [row[c] for c in en_to_x_cols if pd.notna(row.get(c))]\n    en_to_x_avg = sum(en_to_x_vals) / len(en_to_x_vals) if en_to_x_vals else None\n    \n    # X→EN average\n    x_to_en_vals = [row[c] for c in x_to_en_cols if pd.notna(row.get(c))]\n    x_to_en_avg = sum(x_to_en_vals) / len(x_to_en_vals) if x_to_en_vals else None\n    \n    # Determine behavior\n    if x_to_en_avg is not None:\n        if x_to_en_avg >= 90:\n            behavior = \"Query-following\"\n        elif x_to_en_avg <= 10:\n            behavior = \"Context-anchoring\"\n        else:\n            behavior = \"Mixed\"\n    else:\n        behavior = \"--\"\n    \n    summary_data.append({\n        'Model': model_name,\n        'EN→X': en_to_x_avg,\n        'X→EN': x_to_en_avg,\n        'Behavior': behavior\n    })\n\ndf_summary = pd.DataFrame(summary_data)\nprint(df_summary.to_string(index=False))\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LaTeX: MAIN RESULTS TABLE\")\nprint(\"=\" * 80)\nprint(\"\\\\begin{tabular}{@{}lccc@{}}\")\nprint(\"\\\\toprule\")\nprint(\"\\\\textbf{Model} & \\\\textbf{EN→X} & \\\\textbf{X→EN} & \\\\textbf{Behavior} \\\\\\\\\")\nprint(\"\\\\midrule\")\nfor _, row in df_summary.iterrows():\n    en_to_x = f\"{row['EN→X']:.1f}\" if pd.notna(row['EN→X']) else \"--\"\n    x_to_en = f\"{row['X→EN']:.1f}\" if pd.notna(row['X→EN']) else \"--\"\n    # Bold extreme values\n    if pd.notna(row['X→EN']) and (row['X→EN'] >= 95 or row['X→EN'] <= 1):\n        x_to_en = f\"\\\\textbf{{{x_to_en}}}\"\n    print(f\"{row['Model']} & {en_to_x} & {x_to_en} & {row['Behavior']} \\\\\\\\\")\nprint(\"\\\\bottomrule\")\nprint(\"\\\\end{tabular}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Conversation Length Effect (X→EN Fidelity)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nfrom scipy import stats\n\n# New turn ranges: Short (3-5), Medium (7-9), Long (11+)\ndef categorize_length_new(turns):\n    \"\"\"Categorize conversation length with new ranges.\"\"\"\n    if 3 <= turns <= 5:\n        return 'Short'\n    elif 7 <= turns <= 9:\n        return 'Medium'\n    elif turns >= 11:\n        return 'Long'\n    else:\n        return None  # Exclude 2, 6, 10 turns\n\n# Load baseline data with turn counts\nbaseline_data = {}\nwith open('../data/experiments/baseline_en.jsonl') as f:\n    for line in f:\n        item = json.loads(line.strip())\n        baseline_data[item['QUESTION_ID']] = {\n            'turns': len(item['CONVERSATION'])\n        }\n\n# Check turn distribution\nfrom collections import Counter\nturn_counts = [v['turns'] for v in baseline_data.values()]\nprint(\"Turn Distribution in Dataset:\")\nprint(\"=\" * 50)\nfor t, count in sorted(Counter(turn_counts).items()):\n    cat = categorize_length_new(t)\n    cat_str = f\"({cat})\" if cat else \"(excluded)\"\n    print(f\"  {t:2d} turns: {count:3d} {cat_str}\")\n\n# Count by new categories\nshort = sum(1 for t in turn_counts if 3 <= t <= 5)\nmedium = sum(1 for t in turn_counts if 7 <= t <= 9)\nlong = sum(1 for t in turn_counts if t >= 11)\nprint(f\"\\nNew categories:\")\nprint(f\"  Short (3-5 turns):   n={short}\")\nprint(f\"  Medium (7-9 turns):  n={medium}\")\nprint(f\"  Long (11+ turns):    n={long}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def load_responses_with_turns(model, condition):\n    \"\"\"Load responses and extract turn counts.\"\"\"\n    patterns = [\n        f'../results/responses/{model}/responses_{condition}_*.jsonl',\n        f'../results/responses/{model}/responses_{condition}.jsonl',\n    ]\n    for pattern in patterns:\n        files = sorted(glob.glob(pattern))\n        if files:\n            data = []\n            with open(files[-1]) as f:\n                for line in f:\n                    item = json.loads(line.strip())\n                    if item.get('success'):\n                        data.append({\n                            'question_id': item.get('question_id'),\n                            'turn_count': item.get('turn_count'),\n                        })\n            return data\n    return None\n\ndef load_language_eval_by_qid(model, condition):\n    \"\"\"Load language evaluation results by question ID.\"\"\"\n    pattern = f'../results/layer1/{model}/language_eval_{condition}_*.jsonl'\n    files = sorted(glob.glob(pattern))\n    if not files:\n        return None\n    \n    data = {}\n    with open(files[-1]) as f:\n        for line in f:\n            item = json.loads(line.strip())\n            qid = item.get('question_id')\n            data[qid] = item.get('match_status') == 'match'\n    return data\n\ndef compute_fidelity_by_length_new(model):\n    \"\"\"Compute X→EN fidelity by conversation length (new ranges).\"\"\"\n    results = {'Short': {'match': 0, 'total': 0},\n               'Medium': {'match': 0, 'total': 0},\n               'Long': {'match': 0, 'total': 0}}\n    \n    for lang in LANGS:\n        condition = f\"{lang}_to_en\"\n        \n        responses = load_responses_with_turns(model, condition)\n        lang_eval = load_language_eval_by_qid(model, condition)\n        \n        if not responses or not lang_eval:\n            continue\n        \n        for resp in responses:\n            qid = resp['question_id']\n            turns = resp['turn_count']\n            category = categorize_length_new(turns)\n            \n            if category and qid in lang_eval:\n                results[category]['total'] += 1\n                if lang_eval[qid]:\n                    results[category]['match'] += 1\n    \n    return results\n\ndef chi_square_test(results):\n    \"\"\"Run chi-square test for independence between length and fidelity.\"\"\"\n    observed = []\n    for cat in ['Short', 'Medium', 'Long']:\n        match = results[cat]['match']\n        total = results[cat]['total']\n        if total == 0:\n            return None, None\n        observed.append([match, total - match])\n    \n    observed = np.array(observed).T\n    \n    # Check if test is valid\n    if np.any(observed.sum(axis=0) < 5):\n        return None, \"Low counts\"\n    \n    try:\n        chi2, p, dof, expected = stats.chi2_contingency(observed)\n        return chi2, p\n    except:\n        return None, None\n\n# Compute for all models\nlength_results = {}\nfor model_id in MODELS.keys():\n    length_results[model_id] = compute_fidelity_by_length_new(model_id)\n\nprint(\"X→EN Fidelity by Conversation Length (New Ranges)\")\nprint(\"=\" * 80)\nprint(f\"{'Model':<20} {'Short (3-5)':>15} {'Med (7-9)':>15} {'Long (11+)':>15} {'p-value':>12}\")\nprint(\"-\" * 80)\n\nfor model_id, model_name in MODELS.items():\n    res = length_results[model_id]\n    chi2, p = chi_square_test(res)\n    \n    row = []\n    for cat in ['Short', 'Medium', 'Long']:\n        if res[cat]['total'] > 0:\n            pct = res[cat]['match'] / res[cat]['total'] * 100\n            row.append(f\"{pct:.1f}%\")\n        else:\n            row.append(\"--\")\n    \n    if p is not None and isinstance(p, float):\n        p_str = \"<0.001\" if p < 0.001 else f\"{p:.2f}\"\n    else:\n        p_str = \"—\"\n    \n    print(f\"{model_name:<20} {row[0]:>15} {row[1]:>15} {row[2]:>15} {p_str:>12}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# LaTeX output for conversation length table\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LaTeX: CONVERSATION LENGTH TABLE\")\nprint(\"=\" * 80)\nprint(\"\\\\begin{tabular}{@{}lcccc@{}}\")\nprint(\"\\\\toprule\")\nprint(\"\\\\textbf{Model} & \\\\textbf{Short} & \\\\textbf{Med} & \\\\textbf{Long} & \\\\textbf{$p$} \\\\\\\\\")\nprint(\"\\\\midrule\")\n\nfor model_id, model_name in MODELS.items():\n    res = length_results[model_id]\n    chi2, p = chi_square_test(res)\n    \n    vals = []\n    for cat in ['Short', 'Medium', 'Long']:\n        if res[cat]['total'] > 0:\n            pct = res[cat]['match'] / res[cat]['total'] * 100\n            vals.append(f\"{pct:.1f}\\\\%\")\n        else:\n            vals.append(\"0\\\\%\")\n    \n    if p is not None and isinstance(p, float):\n        p_str = \"$<$0.001\" if p < 0.001 else f\"{p:.2f}\"\n    else:\n        p_str = \"—\"\n    \n    print(f\"{model_name} & {vals[0]} & {vals[1]} & {vals[2]} & {p_str} \\\\\\\\\")\n\nprint(\"\\\\bottomrule\")\nprint(\"\\\\end{tabular}\")\nprint()\nprint(\"% Caption: X→EN fidelity by conversation length.\")\nprint(\"% Short: 3--5 turns, Medium: 7--9 turns, Long: 11+ turns.\")\nprint(\"% $p$-values from $\\\\chi^2$ tests.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}