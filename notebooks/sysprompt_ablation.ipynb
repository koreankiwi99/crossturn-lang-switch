{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Prompt Ablation Analysis\n",
    "\n",
    "Tests whether explicit language instructions improve fidelity on switching conditions (EN→X and X→EN).\n",
    "\n",
    "**Settings:**\n",
    "- None: No system prompt\n",
    "- Explicit: \"Always respond in the same language the user uses in their most recent message.\"\n",
    "\n",
    "**Models:** GPT-5, Claude Opus 4.5, Command R+\n",
    "\n",
    "**Conditions:** EN→X and X→EN for DE, ZH, ES, AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:03.755927Z",
     "iopub.status.busy": "2026-02-05T02:35:03.755707Z",
     "iopub.status.idle": "2026-02-05T02:35:04.425111Z",
     "shell.execute_reply": "2026-02-05T02:35:04.424608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['gpt-5', 'claude-opus-4.5', 'command-r-plus']\n",
      "Languages: ['de', 'zh', 'es', 'ar']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to project root\n",
    "#os.chdir(Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent)\n",
    "#print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "MODELS = {\n",
    "    'gpt-5': 'GPT-5',\n",
    "    'claude-opus-4.5': 'Claude Opus 4.5',\n",
    "    'command-r-plus': 'Command R+',\n",
    "}\n",
    "\n",
    "LANGS = ['de', 'zh', 'es', 'ar']\n",
    "DIRECTIONS = ['en_to', 'to_en']  # EN→X and X→EN\n",
    "\n",
    "print(f\"Models: {list(MODELS.keys())}\")\n",
    "print(f\"Languages: {LANGS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sanity Check: Errors and Empty Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.448198Z",
     "iopub.status.busy": "2026-02-05T02:35:04.447895Z",
     "iopub.status.idle": "2026-02-05T02:35:04.613449Z",
     "shell.execute_reply": "2026-02-05T02:35:04.612723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANITY CHECK: Sysprompt Ablation Response Errors\n",
      "================================================================================\n",
      "\n",
      "GPT-5 (with explicit prompt):\n",
      "  en_to_de: OK 182/182\n",
      "  de_to_en: OK 182/182\n",
      "  en_to_zh: OK 182/182\n",
      "  zh_to_en: OK 182/182\n",
      "  en_to_es: OK 182/182\n",
      "  es_to_en: OK 182/182\n",
      "  en_to_ar: OK 182/182\n",
      "  ar_to_en: OK 182/182\n",
      "\n",
      "Claude Opus 4.5 (with explicit prompt):\n",
      "  en_to_de: Empty: 1\n",
      "  de_to_en: OK 182/182\n",
      "  en_to_zh: OK 182/182\n",
      "  zh_to_en: OK 182/182\n",
      "  en_to_es: OK 182/182\n",
      "  es_to_en: OK 182/182\n",
      "  en_to_ar: OK 182/182\n",
      "  ar_to_en: OK 182/182\n",
      "\n",
      "Command R+ (with explicit prompt):\n",
      "  en_to_de: OK 182/182\n",
      "  de_to_en: OK 182/182\n",
      "  en_to_zh: OK 182/182\n",
      "  zh_to_en: OK 182/182\n",
      "  en_to_es: OK 182/182\n",
      "  es_to_en: OK 182/182\n",
      "  en_to_ar: OK 182/182\n",
      "  ar_to_en: OK 182/182\n"
     ]
    }
   ],
   "source": [
    "def check_response_errors(model, condition, with_sysprompt=True):\n",
    "    \"\"\"Check response files for API errors and empty responses.\"\"\"\n",
    "    if with_sysprompt:\n",
    "        pattern = f'../results/sysprompt-ablation/{model}/responses/responses_{condition}_sysprompt_*.jsonl'\n",
    "    else:\n",
    "        pattern = f'../results/responses/{model}/responses_{condition}_*.jsonl'\n",
    "    \n",
    "    files = sorted(glob.glob(pattern))\n",
    "    # Exclude variance runs (run2, run3) - only use primary results\n",
    "    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    stats = {\n",
    "        'total': 0,\n",
    "        'success': 0,\n",
    "        'api_error': 0,\n",
    "        'empty_response': 0,\n",
    "        'parse_error': 0\n",
    "    }\n",
    "\n",
    "    with open(files[-1]) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                stats['total'] += 1\n",
    "                if data.get('success'):\n",
    "                    stats['success'] += 1\n",
    "                    if not data.get('response') or data.get('response', '').strip() == '':\n",
    "                        stats['empty_response'] += 1\n",
    "                else:\n",
    "                    stats['api_error'] += 1\n",
    "            except json.JSONDecodeError:\n",
    "                stats['parse_error'] += 1\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Run sanity check for sysprompt responses\n",
    "print(\"SANITY CHECK: Sysprompt Ablation Response Errors\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_clean = True\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name} (with explicit prompt):\")\n",
    "    for lang in LANGS:\n",
    "        for direction in ['en_to', 'to_en']:\n",
    "            if direction == 'en_to':\n",
    "                condition = f\"en_to_{lang}\"\n",
    "            else:\n",
    "                condition = f\"{lang}_to_en\"\n",
    "            \n",
    "            stats = check_response_errors(model_id, condition, with_sysprompt=True)\n",
    "            if stats:\n",
    "                errors = []\n",
    "                if stats['api_error'] > 0:\n",
    "                    errors.append(f\"API errors: {stats['api_error']}\")\n",
    "                if stats['empty_response'] > 0:\n",
    "                    errors.append(f\"Empty: {stats['empty_response']}\")\n",
    "                if stats['parse_error'] > 0:\n",
    "                    errors.append(f\"Parse errors: {stats['parse_error']}\")\n",
    "                if errors:\n",
    "                    print(f\"  {condition}: {', '.join(errors)}\")\n",
    "                    all_clean = False\n",
    "                else:\n",
    "                    print(f\"  {condition}: OK {stats['success']}/{stats['total']}\")\n",
    "            else:\n",
    "                print(f\"  {condition}: NO FILE\")\n",
    "                all_clean = False\n",
    "\n",
    "if all_clean:\n",
    "    print(\"\\nAll sysprompt response files are clean (no errors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Language Fidelity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.616398Z",
     "iopub.status.busy": "2026-02-05T02:35:04.616143Z",
     "iopub.status.idle": "2026-02-05T02:35:04.621560Z",
     "shell.execute_reply": "2026-02-05T02:35:04.621140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5 EN->DE (with prompt): 98.4%\n"
     ]
    }
   ],
   "source": [
    "def load_language_fidelity(model, condition, with_sysprompt=True):\n",
    "    \"\"\"Load language fidelity from summary file.\"\"\"\n",
    "    if with_sysprompt:\n",
    "        pattern = f'../results/sysprompt-ablation/{model}/layer1/language_summary_{condition}_*.json'\n",
    "    else:\n",
    "        pattern = f'../results/layer1/{model}/language_summary_{condition}_*.json'\n",
    "    \n",
    "    files = sorted(glob.glob(pattern))\n",
    "    # Exclude variance runs (run2, run3) - only use primary results\n",
    "    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n",
    "    if files:\n",
    "        with open(files[-1]) as f:\n",
    "            data = json.load(f)\n",
    "        return data.get('fidelity_rate'), data.get('stats', {})\n",
    "    return None, None\n",
    "\n",
    "# Test\n",
    "fidelity, stats = load_language_fidelity('gpt-5', 'en_to_de', with_sysprompt=True)\n",
    "if fidelity:\n",
    "    print(f\"GPT-5 EN->DE (with prompt): {fidelity:.1f}%\")\n",
    "else:\n",
    "    print(\"No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Missing Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.623792Z",
     "iopub.status.busy": "2026-02-05T02:35:04.623605Z",
     "iopub.status.idle": "2026-02-05T02:35:04.638134Z",
     "shell.execute_reply": "2026-02-05T02:35:04.637729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Status (Sysprompt Ablation):"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "GPT-5:\n",
      "  EN->DE: Layer1=OK (98.4%)\n",
      "  DE->EN: Layer1=OK (94.0%)\n",
      "  EN->ZH: Layer1=OK (98.9%)\n",
      "  ZH->EN: Layer1=OK (93.4%)\n",
      "  EN->ES: Layer1=OK (99.5%)\n",
      "  ES->EN: Layer1=OK (94.0%)\n",
      "  EN->AR: Layer1=OK (98.9%)\n",
      "  AR->EN: Layer1=OK (94.5%)\n",
      "\n",
      "Claude Opus 4.5:\n",
      "  EN->DE: Layer1=OK (96.7%)\n",
      "  DE->EN: Layer1=OK (9.9%)\n",
      "  EN->ZH: Layer1=OK (94.5%)\n",
      "  ZH->EN: Layer1=OK (9.9%)\n",
      "  EN->ES: Layer1=OK (97.3%)\n",
      "  ES->EN: Layer1=OK (5.5%)\n",
      "  EN->AR: Layer1=OK (96.2%)\n",
      "  AR->EN: Layer1=OK (3.3%)\n",
      "\n",
      "Command R+:\n",
      "  EN->DE: Layer1=OK (91.8%)\n",
      "  DE->EN: Layer1=OK (1.1%)\n",
      "  EN->ZH: Layer1=OK (87.4%)\n",
      "  ZH->EN: Layer1=OK (0.5%)\n",
      "  EN->ES: Layer1=OK (96.7%)\n",
      "  ES->EN: Layer1=OK (0.5%)\n",
      "  EN->AR: Layer1=OK (83.0%)\n",
      "  AR->EN: Layer1=OK (0.5%)\n",
      "\n",
      "All evaluations complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation Status (Sysprompt Ablation):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for lang in LANGS:\n",
    "        for direction in ['en_to', 'to_en']:\n",
    "            if direction == 'en_to':\n",
    "                condition = f\"en_to_{lang}\"\n",
    "                display = f\"EN->{lang.upper()}\"\n",
    "            else:\n",
    "                condition = f\"{lang}_to_en\"\n",
    "                display = f\"{lang.upper()}->EN\"\n",
    "            \n",
    "            # Check if response exists\n",
    "            resp_pattern = f'../results/sysprompt-ablation/{model_id}/responses/responses_{condition}_sysprompt_*.jsonl'\n",
    "            resp_files = glob.glob(resp_pattern)\n",
    "            \n",
    "            # Check layer1\n",
    "            fidelity, _ = load_language_fidelity(model_id, condition, with_sysprompt=True)\n",
    "            \n",
    "            if resp_files:\n",
    "                layer1_status = f\"OK ({fidelity:.1f}%)\" if fidelity else \"MISSING\"\n",
    "                print(f\"  {display}: Layer1={layer1_status}\")\n",
    "                if not fidelity:\n",
    "                    missing.append({\n",
    "                        'model': model_id,\n",
    "                        'condition': condition,\n",
    "                        'resp_file': resp_files[-1]\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"  {display}: NO RESPONSES\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nMissing {len(missing)} evaluations\")\n",
    "else:\n",
    "    print(\"\\nAll evaluations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.640562Z",
     "iopub.status.busy": "2026-02-05T02:35:04.640267Z",
     "iopub.status.idle": "2026-02-05T02:35:04.655409Z",
     "shell.execute_reply": "2026-02-05T02:35:04.654817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT ABLATION - Language Fidelity (%)\n",
      "==========================================================================================\n",
      "          Model Condition   Prompt        DE        ZH        ES        AR\n",
      "          GPT-5      EN→X     None 97.802198 99.450549 99.450549 97.802198\n",
      "          GPT-5      EN→X Explicit 98.351648 98.901099 99.450549 98.901099\n",
      "          GPT-5      X→EN     None 93.956044 95.604396 94.505495 96.153846\n",
      "          GPT-5      X→EN Explicit 93.956044 93.406593 93.956044 94.505495\n",
      "Claude Opus 4.5      EN→X     None 96.685083 93.956044 97.252747 96.703297\n",
      "Claude Opus 4.5      EN→X Explicit 96.685083 94.505495 97.252747 96.153846\n",
      "Claude Opus 4.5      X→EN     None 10.439560  9.890110  6.043956  4.395604\n",
      "Claude Opus 4.5      X→EN Explicit  9.890110  9.890110  5.494505  3.296703\n",
      "     Command R+      EN→X     None 91.758242 89.010989 95.604396 80.769231\n",
      "     Command R+      EN→X Explicit 91.758242 87.362637 96.703297 82.967033\n",
      "     Command R+      X→EN     None  1.098901  1.098901  0.549451  0.549451\n",
      "     Command R+      X→EN Explicit  1.098901  0.549451  0.549451  0.549451\n"
     ]
    }
   ],
   "source": [
    "# Build comparison table\n",
    "results = []\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    for direction in ['EN→X', 'X→EN']:\n",
    "        for prompt_type in ['None', 'Explicit']:\n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Condition': direction,\n",
    "                'Prompt': prompt_type\n",
    "            }\n",
    "            \n",
    "            for lang in LANGS:\n",
    "                if direction == 'EN→X':\n",
    "                    condition = f\"en_to_{lang}\"\n",
    "                else:\n",
    "                    condition = f\"{lang}_to_en\"\n",
    "                \n",
    "                with_prompt = (prompt_type == 'Explicit')\n",
    "                fidelity, _ = load_language_fidelity(model_id, condition, with_sysprompt=with_prompt)\n",
    "                \n",
    "                row[lang.upper()] = fidelity\n",
    "            \n",
    "            results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"SYSTEM PROMPT ABLATION - Language Fidelity (%)\")\n",
    "print(\"=\" * 90)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Delta (Explicit - None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.657892Z",
     "iopub.status.busy": "2026-02-05T02:35:04.657678Z",
     "iopub.status.idle": "2026-02-05T02:35:04.667247Z",
     "shell.execute_reply": "2026-02-05T02:35:04.666582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPROVEMENT FROM EXPLICIT PROMPT (Explicit - None)\n",
      "================================================================================\n",
      "\n",
      "GPT-5:\n",
      "  EN→X: DE=  +0.5, ZH=  -0.5, ES=  +0.0, AR=  +1.1\n",
      "  X→EN: DE=  +0.0, ZH=  -2.2, ES=  -0.5, AR=  -1.6\n",
      "\n",
      "Claude Opus 4.5:\n",
      "  EN→X: DE=  +0.0, ZH=  +0.5, ES=  +0.0, AR=  -0.5\n",
      "  X→EN: DE=  -0.5, ZH=  +0.0, ES=  -0.5, AR=  -1.1\n",
      "\n",
      "Command R+:\n",
      "  EN→X: DE=  +0.0, ZH=  -1.6, ES=  +1.1, AR=  +2.2\n",
      "  X→EN: DE=  +0.0, ZH=  -0.5, ES=  +0.0, AR=  +0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute improvement from explicit prompt\n",
    "print(\"IMPROVEMENT FROM EXPLICIT PROMPT (Explicit - None)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for direction in ['EN→X', 'X→EN']:\n",
    "        row = []\n",
    "        for lang in LANGS:\n",
    "            if direction == 'EN→X':\n",
    "                condition = f\"en_to_{lang}\"\n",
    "            else:\n",
    "                condition = f\"{lang}_to_en\"\n",
    "            \n",
    "            fidelity_none, _ = load_language_fidelity(model_id, condition, with_sysprompt=False)\n",
    "            fidelity_explicit, _ = load_language_fidelity(model_id, condition, with_sysprompt=True)\n",
    "            \n",
    "            if fidelity_none is not None and fidelity_explicit is not None:\n",
    "                delta = fidelity_explicit - fidelity_none\n",
    "                sign = \"+\" if delta >= 0 else \"\"\n",
    "                row.append(f\"{sign}{delta:.1f}\")\n",
    "            else:\n",
    "                row.append(\"--\")\n",
    "        \n",
    "        print(f\"  {direction}: DE={row[0]:>6}, ZH={row[1]:>6}, ES={row[2]:>6}, AR={row[3]:>6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Task Accuracy Results (Layer 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.670036Z",
     "iopub.status.busy": "2026-02-05T02:35:04.669812Z",
     "iopub.status.idle": "2026-02-05T02:35:04.684807Z",
     "shell.execute_reply": "2026-02-05T02:35:04.684409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5 EN->DE Task Accuracy (Explicit): 58.2%\n",
      "GPT-5 EN->DE Task Accuracy (None): 57.1%\n"
     ]
    }
   ],
   "source": [
    "def load_task_accuracy(model, condition, with_sysprompt=True):\n",
    "    \"\"\"Load task accuracy from summary or evaluated file.\"\"\"\n",
    "    if with_sysprompt:\n",
    "        summary_pattern = f'../results/sysprompt-ablation/{model}/layer2/summary_{condition}_*.json'\n",
    "        eval_pattern = f'../results/sysprompt-ablation/{model}/layer2/evaluated_{condition}_*.jsonl'\n",
    "    else:\n",
    "        summary_pattern = f'../results/layer2/{model}/summary_{condition}_*.json'\n",
    "        eval_pattern = f'../results/layer2/{model}/evaluated_{condition}_*.jsonl'\n",
    "    \n",
    "    # Try summary first\n",
    "    files = sorted(glob.glob(summary_pattern))\n",
    "    # Exclude variance runs (run2, run3) - only use primary results\n",
    "    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n",
    "    if files:\n",
    "        with open(files[-1]) as f:\n",
    "            data = json.load(f)\n",
    "        return data.get('pass_rate'), data.get('stats', {})\n",
    "    \n",
    "    # Fall back to evaluated file\n",
    "    files = sorted(glob.glob(eval_pattern))\n",
    "    # Exclude variance runs (run2, run3) - only use primary results\n",
    "    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n",
    "    if files:\n",
    "        total, passed = 0, 0\n",
    "        with open(files[-1]) as f:\n",
    "            for line in f:\n",
    "                d = json.loads(line)\n",
    "                total += 1\n",
    "                if d.get('evaluation', {}).get('passed'):\n",
    "                    passed += 1\n",
    "        if total > 0:\n",
    "            return (passed / total) * 100, {'total': total, 'passed': passed}\n",
    "    return None, None\n",
    "\n",
    "# Test\n",
    "accuracy, stats = load_task_accuracy('gpt-5', 'en_to_de', with_sysprompt=True)\n",
    "print(f\"GPT-5 EN->DE Task Accuracy (Explicit): {accuracy:.1f}%\" if accuracy else \"No data\")\n",
    "accuracy, stats = load_task_accuracy('gpt-5', 'en_to_de', with_sysprompt=False)\n",
    "print(f\"GPT-5 EN->DE Task Accuracy (None): {accuracy:.1f}%\" if accuracy else \"No data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check Missing Layer 2 Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.687018Z",
     "iopub.status.busy": "2026-02-05T02:35:04.686865Z",
     "iopub.status.idle": "2026-02-05T02:35:04.696667Z",
     "shell.execute_reply": "2026-02-05T02:35:04.696324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 Evaluation Status (Sysprompt Ablation):\n",
      "================================================================================\n",
      "\n",
      "GPT-5:\n",
      "  EN->DE: Layer2=OK (58.2%)\n",
      "  DE->EN: Layer2=OK (54.9%)\n",
      "  EN->ZH: Layer2=OK (59.9%)\n",
      "  ZH->EN: Layer2=OK (57.7%)\n",
      "  EN->ES: Layer2=OK (58.8%)\n",
      "  ES->EN: Layer2=OK (53.8%)\n",
      "  EN->AR: Layer2=OK (56.0%)\n",
      "  AR->EN: Layer2=OK (53.8%)\n",
      "\n",
      "Claude Opus 4.5:\n",
      "  EN->DE: Layer2=OK (48.4%)\n",
      "  DE->EN: Layer2=OK (48.4%)\n",
      "  EN->ZH: Layer2=OK (52.7%)\n",
      "  ZH->EN: Layer2=OK (51.1%)\n",
      "  EN->ES: Layer2=OK (55.5%)\n",
      "  ES->EN: Layer2=OK (51.6%)\n",
      "  EN->AR: Layer2=OK (54.4%)\n",
      "  AR->EN: Layer2=OK (48.9%)\n",
      "\n",
      "Command R+:\n",
      "  EN->DE: Layer2=OK (14.8%)\n",
      "  DE->EN: Layer2=OK (11.5%)\n",
      "  EN->ZH: Layer2=OK (17.6%)\n",
      "  ZH->EN: Layer2=OK (12.6%)\n",
      "  EN->ES: Layer2=OK (14.8%)\n",
      "  ES->EN: Layer2=OK (12.6%)\n",
      "  EN->AR: Layer2=OK (15.4%)\n",
      "  AR->EN: Layer2=OK (11.0%)\n",
      "\n",
      "All Layer 2 evaluations complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 2 Evaluation Status (Sysprompt Ablation):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_layer2 = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for lang in LANGS:\n",
    "        for direction in ['en_to', 'to_en']:\n",
    "            if direction == 'en_to':\n",
    "                condition = f\"en_to_{lang}\"\n",
    "                display = f\"EN->{lang.upper()}\"\n",
    "            else:\n",
    "                condition = f\"{lang}_to_en\"\n",
    "                display = f\"{lang.upper()}->EN\"\n",
    "            \n",
    "            # Check if response exists\n",
    "            resp_pattern = f'../results/sysprompt-ablation/{model_id}/responses/responses_{condition}_sysprompt_*.jsonl'\n",
    "            resp_files = glob.glob(resp_pattern)\n",
    "            \n",
    "            # Check layer2\n",
    "            accuracy, _ = load_task_accuracy(model_id, condition, with_sysprompt=True)\n",
    "            \n",
    "            if resp_files:\n",
    "                layer2_status = f\"OK ({accuracy:.1f}%)\" if accuracy else \"MISSING\"\n",
    "                print(f\"  {display}: Layer2={layer2_status}\")\n",
    "                if not accuracy:\n",
    "                    missing_layer2.append({\n",
    "                        'model': model_id,\n",
    "                        'condition': condition,\n",
    "                        'resp_file': resp_files[-1]\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"  {display}: NO RESPONSES\")\n",
    "\n",
    "if missing_layer2:\n",
    "    print(f\"\\nMissing {len(missing_layer2)} Layer 2 evaluations\")\n",
    "else:\n",
    "    print(\"\\nAll Layer 2 evaluations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Task Accuracy Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.699268Z",
     "iopub.status.busy": "2026-02-05T02:35:04.699015Z",
     "iopub.status.idle": "2026-02-05T02:35:04.800423Z",
     "shell.execute_reply": "2026-02-05T02:35:04.800212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT ABLATION - Task Accuracy (%)\n",
      "==========================================================================================\n",
      "          Model Condition   Prompt        DE        ZH        ES        AR\n",
      "          GPT-5      EN→X     None 57.142857 59.890110 59.340659 60.439560\n",
      "          GPT-5      EN→X Explicit 58.241758 59.890110 58.791209 56.043956\n",
      "          GPT-5      X→EN     None 55.494505 50.549451 49.450549 54.395604\n",
      "          GPT-5      X→EN Explicit 54.945055 57.692308 53.846154 53.846154\n",
      "Claude Opus 4.5      EN→X     None 49.450549 46.703297 50.549451 48.901099\n",
      "Claude Opus 4.5      EN→X Explicit 48.351648 52.747253 55.494505 54.395604\n",
      "Claude Opus 4.5      X→EN     None 48.351648 47.802198 52.747253 50.549451\n",
      "Claude Opus 4.5      X→EN Explicit 48.351648 51.098901 51.648352 48.901099\n",
      "     Command R+      EN→X     None 15.384615 13.186813 15.384615 15.934066\n",
      "     Command R+      EN→X Explicit 14.835165 17.582418 14.835165 15.384615\n",
      "     Command R+      X→EN     None 12.087912 10.989011 11.538462 10.989011\n",
      "     Command R+      X→EN Explicit 11.538462 12.637363 12.637363 10.989011\n"
     ]
    }
   ],
   "source": [
    "# Build task accuracy comparison table\n",
    "acc_results = []\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    for direction in ['EN→X', 'X→EN']:\n",
    "        for prompt_type in ['None', 'Explicit']:\n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Condition': direction,\n",
    "                'Prompt': prompt_type\n",
    "            }\n",
    "            \n",
    "            for lang in LANGS:\n",
    "                if direction == 'EN→X':\n",
    "                    condition = f\"en_to_{lang}\"\n",
    "                else:\n",
    "                    condition = f\"{lang}_to_en\"\n",
    "                \n",
    "                with_prompt = (prompt_type == 'Explicit')\n",
    "                accuracy, _ = load_task_accuracy(model_id, condition, with_sysprompt=with_prompt)\n",
    "                \n",
    "                row[lang.upper()] = accuracy\n",
    "            \n",
    "            acc_results.append(row)\n",
    "\n",
    "df_acc = pd.DataFrame(acc_results)\n",
    "print(\"SYSTEM PROMPT ABLATION - Task Accuracy (%)\")\n",
    "print(\"=\" * 90)\n",
    "print(df_acc.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Task Accuracy Delta (Explicit - None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:35:04.801597Z",
     "iopub.status.busy": "2026-02-05T02:35:04.801507Z",
     "iopub.status.idle": "2026-02-05T02:35:04.900492Z",
     "shell.execute_reply": "2026-02-05T02:35:04.900260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK ACCURACY IMPROVEMENT FROM EXPLICIT PROMPT (Explicit - None)\n",
      "================================================================================\n",
      "\n",
      "GPT-5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EN→X: DE=  +1.1, ZH=  +0.0, ES=  -0.5, AR=  -4.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X→EN: DE=  -0.5, ZH=  +7.1, ES=  +4.4, AR=  -0.5\n",
      "\n",
      "Claude Opus 4.5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EN→X: DE=  -1.1, ZH=  +6.0, ES=  +4.9, AR=  +5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X→EN: DE=  +0.0, ZH=  +3.3, ES=  -1.1, AR=  -1.6\n",
      "\n",
      "Command R+:\n",
      "  EN→X: DE=  -0.5, ZH=  +4.4, ES=  -0.5, AR=  -0.5\n",
      "  X→EN: DE=  -0.5, ZH=  +1.6, ES=  +1.1, AR=  +0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute task accuracy improvement from explicit prompt\n",
    "print(\"TASK ACCURACY IMPROVEMENT FROM EXPLICIT PROMPT (Explicit - None)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for direction in ['EN→X', 'X→EN']:\n",
    "        row = []\n",
    "        for lang in LANGS:\n",
    "            if direction == 'EN→X':\n",
    "                condition = f\"en_to_{lang}\"\n",
    "            else:\n",
    "                condition = f\"{lang}_to_en\"\n",
    "            \n",
    "            acc_none, _ = load_task_accuracy(model_id, condition, with_sysprompt=False)\n",
    "            acc_explicit, _ = load_task_accuracy(model_id, condition, with_sysprompt=True)\n",
    "            \n",
    "            if acc_none is not None and acc_explicit is not None:\n",
    "                delta = acc_explicit - acc_none\n",
    "                sign = \"+\" if delta >= 0 else \"\"\n",
    "                row.append(f\"{sign}{delta:.1f}\")\n",
    "            else:\n",
    "                row.append(\"--\")\n",
    "        \n",
    "        print(f\"  {direction}: DE={row[0]:>6}, ZH={row[1]:>6}, ES={row[2]:>6}, AR={row[3]:>6}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
