{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# System Prompt Ablation Analysis\n\nTests whether explicit language instructions improve fidelity on switching conditions (EN→X and X→EN).\n\n**Settings:**\n- None: No system prompt\n- Explicit: \"Always respond in the same language the user uses in their most recent message.\"\n\n**Models:** GPT-5, Claude Opus 4.5, Command R+\n\n**Conditions:** EN→X and X→EN for DE, ZH, ES, AR"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:16.159425Z",
     "iopub.status.busy": "2025-12-20T10:17:16.159310Z",
     "iopub.status.idle": "2025-12-20T10:17:16.765437Z",
     "shell.execute_reply": "2025-12-20T10:17:16.765200Z"
    }
   },
   "outputs": [],
   "source": "import os\nimport json\nimport glob\nimport pandas as pd\nfrom pathlib import Path\n\n# Change to project root\n#os.chdir(Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent)\n#print(f\"Working directory: {os.getcwd()}\")\n\nMODELS = {\n    'gpt-5': 'GPT-5',\n    'claude-opus-4.5': 'Claude Opus 4.5',\n    'command-r-plus': 'Command R+',\n}\n\nLANGS = ['de', 'zh', 'es', 'ar']\nDIRECTIONS = ['en_to', 'to_en']  # EN→X and X→EN\n\nprint(f\"Models: {list(MODELS.keys())}\")\nprint(f\"Languages: {LANGS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sanity Check: Errors and Empty Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:16.784115Z",
     "iopub.status.busy": "2025-12-20T10:17:16.783644Z",
     "iopub.status.idle": "2025-12-20T10:17:16.958753Z",
     "shell.execute_reply": "2025-12-20T10:17:16.958479Z"
    }
   },
   "outputs": [],
   "source": "def check_response_errors(model, condition, with_sysprompt=True):\n    \"\"\"Check response files for API errors and empty responses.\"\"\"\n    if with_sysprompt:\n        pattern = f'../results/sysprompt-ablation/{model}/responses/responses_{condition}_sysprompt_*.jsonl'\n    else:\n        pattern = f'../results/responses/{model}/responses_{condition}_*.jsonl'\n    \n    files = sorted(glob.glob(pattern))\n    # Exclude variance runs (run2, run3) - only use primary results\n    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n    if not files:\n        return None\n\n    stats = {\n        'total': 0,\n        'success': 0,\n        'api_error': 0,\n        'empty_response': 0,\n        'parse_error': 0\n    }\n\n    with open(files[-1]) as f:\n        for line in f:\n            try:\n                data = json.loads(line.strip())\n                stats['total'] += 1\n                if data.get('success'):\n                    stats['success'] += 1\n                    if not data.get('response') or data.get('response', '').strip() == '':\n                        stats['empty_response'] += 1\n                else:\n                    stats['api_error'] += 1\n            except json.JSONDecodeError:\n                stats['parse_error'] += 1\n\n    return stats\n\n# Run sanity check for sysprompt responses\nprint(\"SANITY CHECK: Sysprompt Ablation Response Errors\")\nprint(\"=\" * 80)\n\nall_clean = True\nfor model_id, model_name in MODELS.items():\n    print(f\"\\n{model_name} (with explicit prompt):\")\n    for lang in LANGS:\n        for direction in ['en_to', 'to_en']:\n            if direction == 'en_to':\n                condition = f\"en_to_{lang}\"\n            else:\n                condition = f\"{lang}_to_en\"\n            \n            stats = check_response_errors(model_id, condition, with_sysprompt=True)\n            if stats:\n                errors = []\n                if stats['api_error'] > 0:\n                    errors.append(f\"API errors: {stats['api_error']}\")\n                if stats['empty_response'] > 0:\n                    errors.append(f\"Empty: {stats['empty_response']}\")\n                if stats['parse_error'] > 0:\n                    errors.append(f\"Parse errors: {stats['parse_error']}\")\n                if errors:\n                    print(f\"  {condition}: {', '.join(errors)}\")\n                    all_clean = False\n                else:\n                    print(f\"  {condition}: OK {stats['success']}/{stats['total']}\")\n            else:\n                print(f\"  {condition}: NO FILE\")\n                all_clean = False\n\nif all_clean:\n    print(\"\\nAll sysprompt response files are clean (no errors)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Language Fidelity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:16.960170Z",
     "iopub.status.busy": "2025-12-20T10:17:16.960087Z",
     "iopub.status.idle": "2025-12-20T10:17:16.963159Z",
     "shell.execute_reply": "2025-12-20T10:17:16.962962Z"
    }
   },
   "outputs": [],
   "source": "def load_language_fidelity(model, condition, with_sysprompt=True):\n    \"\"\"Load language fidelity from summary file.\"\"\"\n    if with_sysprompt:\n        pattern = f'../results/sysprompt-ablation/{model}/layer1/language_summary_{condition}_*.json'\n    else:\n        pattern = f'../results/layer1/{model}/language_summary_{condition}_*.json'\n    \n    files = sorted(glob.glob(pattern))\n    # Exclude variance runs (run2, run3) - only use primary results\n    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n    if files:\n        with open(files[-1]) as f:\n            data = json.load(f)\n        return data.get('fidelity_rate'), data.get('stats', {})\n    return None, None\n\n# Test\nfidelity, stats = load_language_fidelity('gpt-5', 'en_to_de', with_sysprompt=True)\nif fidelity:\n    print(f\"GPT-5 EN->DE (with prompt): {fidelity:.1f}%\")\nelse:\n    print(\"No data found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Missing Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:16.964376Z",
     "iopub.status.busy": "2025-12-20T10:17:16.964298Z",
     "iopub.status.idle": "2025-12-20T10:17:16.973492Z",
     "shell.execute_reply": "2025-12-20T10:17:16.973290Z"
    }
   },
   "outputs": [],
   "source": "print(\"Evaluation Status (Sysprompt Ablation):\")\nprint(\"=\" * 80)\n\nmissing = []\nfor model_id, model_name in MODELS.items():\n    print(f\"\\n{model_name}:\")\n    for lang in LANGS:\n        for direction in ['en_to', 'to_en']:\n            if direction == 'en_to':\n                condition = f\"en_to_{lang}\"\n                display = f\"EN->{lang.upper()}\"\n            else:\n                condition = f\"{lang}_to_en\"\n                display = f\"{lang.upper()}->EN\"\n            \n            # Check if response exists\n            resp_pattern = f'../results/sysprompt-ablation/{model_id}/responses/responses_{condition}_sysprompt_*.jsonl'\n            resp_files = glob.glob(resp_pattern)\n            \n            # Check layer1\n            fidelity, _ = load_language_fidelity(model_id, condition, with_sysprompt=True)\n            \n            if resp_files:\n                layer1_status = f\"OK ({fidelity:.1f}%)\" if fidelity else \"MISSING\"\n                print(f\"  {display}: Layer1={layer1_status}\")\n                if not fidelity:\n                    missing.append({\n                        'model': model_id,\n                        'condition': condition,\n                        'resp_file': resp_files[-1]\n                    })\n            else:\n                print(f\"  {display}: NO RESPONSES\")\n\nif missing:\n    print(f\"\\nMissing {len(missing)} evaluations\")\nelse:\n    print(\"\\nAll evaluations complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:16.974602Z",
     "iopub.status.busy": "2025-12-20T10:17:16.974518Z",
     "iopub.status.idle": "2025-12-20T10:17:16.988510Z",
     "shell.execute_reply": "2025-12-20T10:17:16.988306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT ABLATION - Language Fidelity (%)\n",
      "==========================================================================================\n",
      "     Model Condition   Prompt        DE        ZH        ES        AR\n",
      "     GPT-5      EN→X     None 97.802198 99.450549 99.450549 97.802198\n",
      "     GPT-5      EN→X Explicit 98.351648 98.901099 99.450549 98.901099\n",
      "     GPT-5      X→EN     None 93.956044 95.604396 94.505495 96.153846\n",
      "     GPT-5      X→EN Explicit 93.956044 93.406593 93.956044 94.505495\n",
      "Claude 4.5      EN→X     None 96.685083 93.956044 97.252747 96.703297\n",
      "Claude 4.5      EN→X Explicit 96.685083 94.505495 97.252747 96.153846\n",
      "Claude 4.5      X→EN     None 10.439560  9.890110  6.043956  4.395604\n",
      "Claude 4.5      X→EN Explicit  9.890110  9.890110  5.494505  3.296703\n",
      "Command R+      EN→X     None 91.758242 89.010989 95.604396 80.769231\n",
      "Command R+      EN→X Explicit 91.758242 87.362637 96.703297 82.967033\n",
      "Command R+      X→EN     None  1.098901  1.098901  0.549451  0.549451\n",
      "Command R+      X→EN Explicit  1.098901  0.549451  0.549451  0.549451\n"
     ]
    }
   ],
   "source": [
    "# Build comparison table\n",
    "results = []\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    for direction in ['EN→X', 'X→EN']:\n",
    "        for prompt_type in ['None', 'Explicit']:\n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Condition': direction,\n",
    "                'Prompt': prompt_type\n",
    "            }\n",
    "            \n",
    "            for lang in LANGS:\n",
    "                if direction == 'EN→X':\n",
    "                    condition = f\"en_to_{lang}\"\n",
    "                else:\n",
    "                    condition = f\"{lang}_to_en\"\n",
    "                \n",
    "                with_prompt = (prompt_type == 'Explicit')\n",
    "                fidelity, _ = load_language_fidelity(model_id, condition, with_sysprompt=with_prompt)\n",
    "                \n",
    "                row[lang.upper()] = fidelity\n",
    "            \n",
    "            results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"SYSTEM PROMPT ABLATION - Language Fidelity (%)\")\n",
    "print(\"=\" * 90)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Delta (Explicit - None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:16.989812Z",
     "iopub.status.busy": "2025-12-20T10:17:16.989729Z",
     "iopub.status.idle": "2025-12-20T10:17:16.994532Z",
     "shell.execute_reply": "2025-12-20T10:17:16.994360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPROVEMENT FROM EXPLICIT PROMPT (Explicit - None)\n",
      "================================================================================\n",
      "\n",
      "GPT-5:\n",
      "  EN→X: DE=  +0.5, ZH=  -0.5, ES=  +0.0, AR=  +1.1\n",
      "  X→EN: DE=  +0.0, ZH=  -2.2, ES=  -0.5, AR=  -1.6\n",
      "\n",
      "Claude 4.5:\n",
      "  EN→X: DE=  +0.0, ZH=  +0.5, ES=  +0.0, AR=  -0.5\n",
      "  X→EN: DE=  -0.5, ZH=  +0.0, ES=  -0.5, AR=  -1.1\n",
      "\n",
      "Command R+:\n",
      "  EN→X: DE=  +0.0, ZH=  -1.6, ES=  +1.1, AR=  +2.2\n",
      "  X→EN: DE=  +0.0, ZH=  -0.5, ES=  +0.0, AR=  +0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute improvement from explicit prompt\n",
    "print(\"IMPROVEMENT FROM EXPLICIT PROMPT (Explicit - None)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for direction in ['EN→X', 'X→EN']:\n",
    "        row = []\n",
    "        for lang in LANGS:\n",
    "            if direction == 'EN→X':\n",
    "                condition = f\"en_to_{lang}\"\n",
    "            else:\n",
    "                condition = f\"{lang}_to_en\"\n",
    "            \n",
    "            fidelity_none, _ = load_language_fidelity(model_id, condition, with_sysprompt=False)\n",
    "            fidelity_explicit, _ = load_language_fidelity(model_id, condition, with_sysprompt=True)\n",
    "            \n",
    "            if fidelity_none is not None and fidelity_explicit is not None:\n",
    "                delta = fidelity_explicit - fidelity_none\n",
    "                sign = \"+\" if delta >= 0 else \"\"\n",
    "                row.append(f\"{sign}{delta:.1f}\")\n",
    "            else:\n",
    "                row.append(\"--\")\n",
    "        \n",
    "        print(f\"  {direction}: DE={row[0]:>6}, ZH={row[1]:>6}, ES={row[2]:>6}, AR={row[3]:>6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Task Accuracy Results (Layer 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:17.001436Z",
     "iopub.status.busy": "2025-12-20T10:17:17.001355Z",
     "iopub.status.idle": "2025-12-20T10:17:17.012597Z",
     "shell.execute_reply": "2025-12-20T10:17:17.012384Z"
    }
   },
   "outputs": [],
   "source": "def load_task_accuracy(model, condition, with_sysprompt=True):\n    \"\"\"Load task accuracy from summary or evaluated file.\"\"\"\n    if with_sysprompt:\n        summary_pattern = f'../results/sysprompt-ablation/{model}/layer2/summary_{condition}_*.json'\n        eval_pattern = f'../results/sysprompt-ablation/{model}/layer2/evaluated_{condition}_*.jsonl'\n    else:\n        summary_pattern = f'../results/layer2/{model}/summary_{condition}_*.json'\n        eval_pattern = f'../results/layer2/{model}/evaluated_{condition}_*.jsonl'\n    \n    # Try summary first\n    files = sorted(glob.glob(summary_pattern))\n    # Exclude variance runs (run2, run3) - only use primary results\n    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n    if files:\n        with open(files[-1]) as f:\n            data = json.load(f)\n        return data.get('pass_rate'), data.get('stats', {})\n    \n    # Fall back to evaluated file\n    files = sorted(glob.glob(eval_pattern))\n    # Exclude variance runs (run2, run3) - only use primary results\n    files = [f for f in files if '_run2_' not in f and '_run3_' not in f]\n    if files:\n        total, passed = 0, 0\n        with open(files[-1]) as f:\n            for line in f:\n                d = json.loads(line)\n                total += 1\n                if d.get('evaluation', {}).get('passed'):\n                    passed += 1\n        if total > 0:\n            return (passed / total) * 100, {'total': total, 'passed': passed}\n    return None, None\n\n# Test\naccuracy, stats = load_task_accuracy('gpt-5', 'en_to_de', with_sysprompt=True)\nprint(f\"GPT-5 EN->DE Task Accuracy (Explicit): {accuracy:.1f}%\" if accuracy else \"No data\")\naccuracy, stats = load_task_accuracy('gpt-5', 'en_to_de', with_sysprompt=False)\nprint(f\"GPT-5 EN->DE Task Accuracy (None): {accuracy:.1f}%\" if accuracy else \"No data\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check Missing Layer 2 Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:17.013844Z",
     "iopub.status.busy": "2025-12-20T10:17:17.013778Z",
     "iopub.status.idle": "2025-12-20T10:17:17.022630Z",
     "shell.execute_reply": "2025-12-20T10:17:17.022425Z"
    }
   },
   "outputs": [],
   "source": "print(\"Layer 2 Evaluation Status (Sysprompt Ablation):\")\nprint(\"=\" * 80)\n\nmissing_layer2 = []\nfor model_id, model_name in MODELS.items():\n    print(f\"\\n{model_name}:\")\n    for lang in LANGS:\n        for direction in ['en_to', 'to_en']:\n            if direction == 'en_to':\n                condition = f\"en_to_{lang}\"\n                display = f\"EN->{lang.upper()}\"\n            else:\n                condition = f\"{lang}_to_en\"\n                display = f\"{lang.upper()}->EN\"\n            \n            # Check if response exists\n            resp_pattern = f'../results/sysprompt-ablation/{model_id}/responses/responses_{condition}_sysprompt_*.jsonl'\n            resp_files = glob.glob(resp_pattern)\n            \n            # Check layer2\n            accuracy, _ = load_task_accuracy(model_id, condition, with_sysprompt=True)\n            \n            if resp_files:\n                layer2_status = f\"OK ({accuracy:.1f}%)\" if accuracy else \"MISSING\"\n                print(f\"  {display}: Layer2={layer2_status}\")\n                if not accuracy:\n                    missing_layer2.append({\n                        'model': model_id,\n                        'condition': condition,\n                        'resp_file': resp_files[-1]\n                    })\n            else:\n                print(f\"  {display}: NO RESPONSES\")\n\nif missing_layer2:\n    print(f\"\\nMissing {len(missing_layer2)} Layer 2 evaluations\")\nelse:\n    print(\"\\nAll Layer 2 evaluations complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Task Accuracy Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:17.023916Z",
     "iopub.status.busy": "2025-12-20T10:17:17.023829Z",
     "iopub.status.idle": "2025-12-20T10:17:17.154950Z",
     "shell.execute_reply": "2025-12-20T10:17:17.154726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT ABLATION - Task Accuracy (%)\n",
      "==========================================================================================\n",
      "     Model Condition   Prompt        DE        ZH        ES        AR\n",
      "     GPT-5      EN→X     None 57.142857 59.890110 59.340659 60.439560\n",
      "     GPT-5      EN→X Explicit 58.241758 59.890110 58.791209 56.043956\n",
      "     GPT-5      X→EN     None 55.494505 50.549451 49.450549 54.395604\n",
      "     GPT-5      X→EN Explicit 54.945055 57.692308 53.846154 53.846154\n",
      "Claude 4.5      EN→X     None 49.450549 46.703297 50.549451 48.901099\n",
      "Claude 4.5      EN→X Explicit 48.351648 52.747253 55.494505 54.395604\n",
      "Claude 4.5      X→EN     None 48.351648 47.802198 52.747253 50.549451\n",
      "Claude 4.5      X→EN Explicit 48.351648 51.098901 51.648352 48.901099\n",
      "Command R+      EN→X     None 15.384615 13.186813 15.384615 15.934066\n",
      "Command R+      EN→X Explicit 14.835165 17.582418 14.835165 15.384615\n",
      "Command R+      X→EN     None 12.087912 10.989011 11.538462 10.989011\n",
      "Command R+      X→EN Explicit 11.538462 12.637363 12.637363 10.989011\n"
     ]
    }
   ],
   "source": [
    "# Build task accuracy comparison table\n",
    "acc_results = []\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    for direction in ['EN→X', 'X→EN']:\n",
    "        for prompt_type in ['None', 'Explicit']:\n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Condition': direction,\n",
    "                'Prompt': prompt_type\n",
    "            }\n",
    "            \n",
    "            for lang in LANGS:\n",
    "                if direction == 'EN→X':\n",
    "                    condition = f\"en_to_{lang}\"\n",
    "                else:\n",
    "                    condition = f\"{lang}_to_en\"\n",
    "                \n",
    "                with_prompt = (prompt_type == 'Explicit')\n",
    "                accuracy, _ = load_task_accuracy(model_id, condition, with_sysprompt=with_prompt)\n",
    "                \n",
    "                row[lang.upper()] = accuracy\n",
    "            \n",
    "            acc_results.append(row)\n",
    "\n",
    "df_acc = pd.DataFrame(acc_results)\n",
    "print(\"SYSTEM PROMPT ABLATION - Task Accuracy (%)\")\n",
    "print(\"=\" * 90)\n",
    "print(df_acc.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Task Accuracy Delta (Explicit - None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T10:17:17.156224Z",
     "iopub.status.busy": "2025-12-20T10:17:17.156134Z",
     "iopub.status.idle": "2025-12-20T10:17:17.265576Z",
     "shell.execute_reply": "2025-12-20T10:17:17.265322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK ACCURACY IMPROVEMENT FROM EXPLICIT PROMPT (Explicit - None)\n",
      "================================================================================\n",
      "\n",
      "GPT-5:\n",
      "  EN→X: DE=  +1.1, ZH=  +0.0, ES=  -0.5, AR=  -4.4\n",
      "  X→EN: DE=  -0.5, ZH=  +7.1, ES=  +4.4, AR=  -0.5\n",
      "\n",
      "Claude 4.5:\n",
      "  EN→X: DE=  -1.1, ZH=  +6.0, ES=  +4.9, AR=  +5.5\n",
      "  X→EN: DE=  +0.0, ZH=  +3.3, ES=  -1.1, AR=  -1.6\n",
      "\n",
      "Command R+:\n",
      "  EN→X: DE=  -0.5, ZH=  +4.4, ES=  -0.5, AR=  -0.5\n",
      "  X→EN: DE=  -0.5, ZH=  +1.6, ES=  +1.1, AR=  +0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute task accuracy improvement from explicit prompt\n",
    "print(\"TASK ACCURACY IMPROVEMENT FROM EXPLICIT PROMPT (Explicit - None)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for direction in ['EN→X', 'X→EN']:\n",
    "        row = []\n",
    "        for lang in LANGS:\n",
    "            if direction == 'EN→X':\n",
    "                condition = f\"en_to_{lang}\"\n",
    "            else:\n",
    "                condition = f\"{lang}_to_en\"\n",
    "            \n",
    "            acc_none, _ = load_task_accuracy(model_id, condition, with_sysprompt=False)\n",
    "            acc_explicit, _ = load_task_accuracy(model_id, condition, with_sysprompt=True)\n",
    "            \n",
    "            if acc_none is not None and acc_explicit is not None:\n",
    "                delta = acc_explicit - acc_none\n",
    "                sign = \"+\" if delta >= 0 else \"\"\n",
    "                row.append(f\"{sign}{delta:.1f}\")\n",
    "            else:\n",
    "                row.append(\"--\")\n",
    "        \n",
    "        print(f\"  {direction}: DE={row[0]:>6}, ZH={row[1]:>6}, ES={row[2]:>6}, AR={row[3]:>6}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}