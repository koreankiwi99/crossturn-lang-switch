{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Lingual Transfer (X→Y) Analysis\n",
    "\n",
    "Analyzes language switching between non-English languages to examine whether English has a privileged role.\n",
    "\n",
    "Test pairs:\n",
    "- ZH→DE (Chinese context, German query)\n",
    "- DE→ZH (German context, Chinese query)\n",
    "- ES→AR (Spanish context, Arabic query)\n",
    "- AR→ES (Arabic context, Spanish query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/kyuheekim/codeswitching-apertus\n",
      "Models: ['gpt-5', 'claude-opus-4.5']\n",
      "Cross-lingual pairs: [('zh', 'de'), ('de', 'zh'), ('es', 'ar'), ('ar', 'es')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to project root\n",
    "os.chdir(Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "MODELS = {\n",
    "    'gpt-5': 'GPT-5',\n",
    "    'claude-opus-4.5': 'Claude 4.5',\n",
    "}\n",
    "\n",
    "# Cross-lingual pairs: (context_lang, query_lang) -> expected_response_lang\n",
    "CROSS_LINGUAL_PAIRS = [\n",
    "    ('zh', 'de'),  # ZH→DE: Chinese context, German query, expect German response\n",
    "    ('de', 'zh'),  # DE→ZH: German context, Chinese query, expect Chinese response\n",
    "    ('es', 'ar'),  # ES→AR: Spanish context, Arabic query, expect Arabic response\n",
    "    ('ar', 'es'),  # AR→ES: Arabic context, Spanish query, expect Spanish response\n",
    "]\n",
    "\n",
    "print(f\"Models: {list(MODELS.keys())}\")\n",
    "print(f\"Cross-lingual pairs: {CROSS_LINGUAL_PAIRS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Task Accuracy (Layer 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5 ZH→DE: 98/182 = 53.8%\n"
     ]
    }
   ],
   "source": [
    "def load_task_accuracy(model, from_lang, to_lang):\n",
    "    \"\"\"Load task accuracy from evaluated JSONL file.\"\"\"\n",
    "    condition = f\"{from_lang}_to_{to_lang}\"\n",
    "    \n",
    "    # Try cross-lingual directory first\n",
    "    patterns = [\n",
    "        f'results/cross-lingual/{model}/evaluated_{condition}_*.jsonl',\n",
    "        f'results/cross-lingual/{model}/evaluated_{condition}.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        if files:\n",
    "            stats = {'total': 0, 'passed': 0, 'failed': 0, 'error': 0}\n",
    "            with open(files[-1]) as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        stats['total'] += 1\n",
    "                        if data.get('evaluation', {}).get('passed'):\n",
    "                            stats['passed'] += 1\n",
    "                        else:\n",
    "                            stats['failed'] += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        stats['error'] += 1\n",
    "            return stats\n",
    "    return None\n",
    "\n",
    "# Test loading\n",
    "test = load_task_accuracy('gpt-5', 'zh', 'de')\n",
    "if test:\n",
    "    print(f\"GPT-5 ZH→DE: {test['passed']}/{test['total']} = {test['passed']/test['total']*100:.1f}%\")\n",
    "else:\n",
    "    print(\"No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Language Fidelity (Layer 1)\n",
    "\n",
    "For cross-lingual pairs, the expected response language is the query language (the target of the switch).\n",
    "- ZH→DE: expect German (de)\n",
    "- DE→ZH: expect Chinese (zh)\n",
    "- ES→AR: expect Arabic (ar)\n",
    "- AR→ES: expect Spanish (es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5 ZH→DE: 96.2%\n",
      "GPT-5 DE→ZH: 97.3%\n",
      "GPT-5 ES→AR: 96.2%\n",
      "GPT-5 AR→ES: 98.4%\n"
     ]
    }
   ],
   "source": [
    "def load_language_fidelity(model, from_lang, to_lang):\n",
    "    \"\"\"Load language fidelity from language_eval JSONL file.\n",
    "    \n",
    "    Expected response language is the query language (to_lang).\n",
    "    \"\"\"\n",
    "    condition = f\"{from_lang}_to_{to_lang}\"\n",
    "    expected_lang = to_lang  # Response should be in query language\n",
    "    \n",
    "    # Try to find language eval file with correct expected language\n",
    "    patterns = [\n",
    "        f'results/cross-lingual/{model}/language_eval_{condition}_*.jsonl',\n",
    "        f'results/cross-lingual/{model}/language_eval_{expected_lang}_*.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        if files:\n",
    "            # Check if file has correct expected language\n",
    "            stats = {'total': 0, 'match': 0, 'mismatch': 0, 'error': 0}\n",
    "            correct_file = None\n",
    "            \n",
    "            for f_path in files:\n",
    "                with open(f_path) as f:\n",
    "                    first_line = f.readline()\n",
    "                    if first_line:\n",
    "                        data = json.loads(first_line)\n",
    "                        if data.get('expected_language') == expected_lang:\n",
    "                            correct_file = f_path\n",
    "                            break\n",
    "            \n",
    "            if correct_file:\n",
    "                with open(correct_file) as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            data = json.loads(line.strip())\n",
    "                            stats['total'] += 1\n",
    "                            status = data.get('match_status', '')\n",
    "                            if status == 'match':\n",
    "                                stats['match'] += 1\n",
    "                            elif status == 'mismatch':\n",
    "                                stats['mismatch'] += 1\n",
    "                            else:\n",
    "                                stats['error'] += 1\n",
    "                        except json.JSONDecodeError:\n",
    "                            stats['error'] += 1\n",
    "                return stats\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test\n",
    "for from_l, to_l in CROSS_LINGUAL_PAIRS:\n",
    "    stats = load_language_fidelity('gpt-5', from_l, to_l)\n",
    "    if stats:\n",
    "        fidelity = stats['match'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "        print(f\"GPT-5 {from_l.upper()}→{to_l.upper()}: {fidelity:.1f}%\")\n",
    "    else:\n",
    "        print(f\"GPT-5 {from_l.upper()}→{to_l.upper()}: No data (need to run language eval)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Results Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK ACCURACY (%)\n",
      "================================================================================\n",
      "            ZH→DE  DE→ZH  ES→AR  AR→ES\n",
      "Model                                 \n",
      "GPT-5        53.8   52.2   56.6   51.6\n",
      "Claude 4.5   47.3   46.2   51.1   48.9\n"
     ]
    }
   ],
   "source": [
    "# Build task accuracy table\n",
    "accuracy_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for from_l, to_l in CROSS_LINGUAL_PAIRS:\n",
    "        col_name = f\"{from_l.upper()}→{to_l.upper()}\"\n",
    "        stats = load_task_accuracy(model_id, from_l, to_l)\n",
    "        if stats and stats['total'] > 0:\n",
    "            row[col_name] = stats['passed'] / stats['total'] * 100\n",
    "        else:\n",
    "            row[col_name] = None\n",
    "    accuracy_data.append(row)\n",
    "\n",
    "df_accuracy = pd.DataFrame(accuracy_data).set_index('Model')\n",
    "print(\"TASK ACCURACY (%)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_accuracy.round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUAGE FIDELITY (%)\n",
      "================================================================================\n",
      "            ZH→DE  DE→ZH  ES→AR  AR→ES\n",
      "Model                                 \n",
      "GPT-5        96.2   97.3   96.2   98.4\n",
      "Claude 4.5   64.3   35.2   81.3   19.2\n"
     ]
    }
   ],
   "source": [
    "# Build language fidelity table\n",
    "fidelity_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for from_l, to_l in CROSS_LINGUAL_PAIRS:\n",
    "        col_name = f\"{from_l.upper()}→{to_l.upper()}\"\n",
    "        stats = load_language_fidelity(model_id, from_l, to_l)\n",
    "        if stats and stats['total'] > 0:\n",
    "            row[col_name] = stats['match'] / stats['total'] * 100\n",
    "        else:\n",
    "            row[col_name] = None\n",
    "    fidelity_data.append(row)\n",
    "\n",
    "df_fidelity = pd.DataFrame(fidelity_data).set_index('Model')\n",
    "print(\"LANGUAGE FIDELITY (%)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_fidelity.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sanity Check: Errors and Empty Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANITY CHECK: Response Errors\n",
      "================================================================================\n",
      "\n",
      "GPT-5:\n",
      "  zh_to_de: ✓ 182/182 OK\n",
      "  de_to_zh: ✓ 182/182 OK\n",
      "  es_to_ar: ✓ 182/182 OK\n",
      "  ar_to_es: ✓ 182/182 OK\n",
      "\n",
      "Claude 4.5:\n",
      "  zh_to_de: Empty: 1\n",
      "  de_to_zh: ✓ 182/182 OK\n",
      "  es_to_ar: ✓ 182/182 OK\n",
      "  ar_to_es: ✓ 182/182 OK\n"
     ]
    }
   ],
   "source": [
    "def check_response_errors(model, from_lang, to_lang):\n",
    "    \"\"\"Check response files for API errors and empty responses.\"\"\"\n",
    "    condition = f\"{from_lang}_to_{to_lang}\"\n",
    "    pattern = f'results/cross-lingual/{model}/responses_{condition}_*.jsonl'\n",
    "    files = sorted(glob.glob(pattern))\n",
    "\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    stats = {\n",
    "        'total': 0,\n",
    "        'success': 0,\n",
    "        'api_error': 0,\n",
    "        'empty_response': 0,\n",
    "        'parse_error': 0\n",
    "    }\n",
    "\n",
    "    with open(files[-1]) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                stats['total'] += 1\n",
    "                if data.get('success'):\n",
    "                    stats['success'] += 1\n",
    "                    if not data.get('response') or data.get('response', '').strip() == '':\n",
    "                        stats['empty_response'] += 1\n",
    "                else:\n",
    "                    stats['api_error'] += 1\n",
    "            except json.JSONDecodeError:\n",
    "                stats['parse_error'] += 1\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Run sanity check\n",
    "print(\"SANITY CHECK: Response Errors\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_clean = True\n",
    "for model_id, model_name in MODELS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    has_errors = False\n",
    "    for from_l, to_l in CROSS_LINGUAL_PAIRS:\n",
    "        condition = f\"{from_l}_to_{to_l}\"\n",
    "        stats = check_response_errors(model_id, from_l, to_l)\n",
    "        if stats:\n",
    "            errors = []\n",
    "            if stats['api_error'] > 0:\n",
    "                errors.append(f\"API errors: {stats['api_error']}\")\n",
    "            if stats['empty_response'] > 0:\n",
    "                errors.append(f\"Empty: {stats['empty_response']}\")\n",
    "            if stats['parse_error'] > 0:\n",
    "                errors.append(f\"Parse errors: {stats['parse_error']}\")\n",
    "            if errors:\n",
    "                print(f\"  {condition}: {', '.join(errors)}\")\n",
    "                has_errors = True\n",
    "                all_clean = False\n",
    "            else:\n",
    "                print(f\"  {condition}: ✓ {stats['success']}/{stats['total']} OK\")\n",
    "        else:\n",
    "            print(f\"  {condition}: NO FILE\")\n",
    "            all_clean = False\n",
    "\n",
    "if all_clean:\n",
    "    print(\"\\n✓ All response files are clean (no errors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LaTeX Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX: LANGUAGE FIDELITY (X→Y)\n",
      "================================================================================\n",
      "GPT-5 & 96.2 & 97.3 & 96.2 & 98.4 \\\\\n",
      "Claude 4.5 & 64.3 & 35.2 & 81.3 & 19.2 \\\\\n",
      "\n",
      "LaTeX: TASK ACCURACY (X→Y)\n",
      "================================================================================\n",
      "GPT-5 & 53.8 & 52.2 & 56.6 & 51.6 \\\\\n",
      "Claude 4.5 & 47.3 & 46.2 & 51.1 & 48.9 \\\\\n"
     ]
    }
   ],
   "source": [
    "def fmt(val):\n",
    "    if val is None or pd.isna(val):\n",
    "        return '--'\n",
    "    return f'{val:.1f}'\n",
    "\n",
    "cols = [f\"{f.upper()}→{t.upper()}\" for f, t in CROSS_LINGUAL_PAIRS]\n",
    "\n",
    "print(\"LaTeX: LANGUAGE FIDELITY (X→Y)\")\n",
    "print(\"=\"*80)\n",
    "for _, row in df_fidelity.iterrows():\n",
    "    vals = [fmt(row.get(c)) for c in cols]\n",
    "    print(f\"{row.name} & {' & '.join(vals)} \\\\\\\\\")\n",
    "\n",
    "print(\"\\nLaTeX: TASK ACCURACY (X→Y)\")\n",
    "print(\"=\"*80)\n",
    "for _, row in df_accuracy.iterrows():\n",
    "    vals = [fmt(row.get(c)) for c in cols]\n",
    "    print(f\"{row.name} & {' & '.join(vals)} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Language Evaluation for Missing Conditions\n",
    "\n",
    "If language evaluation is missing for some conditions, run them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All language evaluations are complete!\n"
     ]
    }
   ],
   "source": [
    "# Find conditions that need language evaluation\n",
    "missing_evals = []\n",
    "\n",
    "for model_id, model_name in MODELS.items():\n",
    "    for from_l, to_l in CROSS_LINGUAL_PAIRS:\n",
    "        condition = f\"{from_l}_to_{to_l}\"\n",
    "        expected_lang = to_l\n",
    "        \n",
    "        # Check if response file exists\n",
    "        pattern = f'results/cross-lingual/{model_id}/responses_{condition}_*.jsonl'\n",
    "        response_files = glob.glob(pattern)\n",
    "        \n",
    "        if not response_files:\n",
    "            continue\n",
    "            \n",
    "        # Check if correct language eval exists\n",
    "        stats = load_language_fidelity(model_id, from_l, to_l)\n",
    "        if not stats:\n",
    "            missing_evals.append({\n",
    "                'model': model_id,\n",
    "                'response_file': response_files[-1],\n",
    "                'expected_lang': expected_lang,\n",
    "                'condition': condition,\n",
    "            })\n",
    "\n",
    "if missing_evals:\n",
    "    print(\"Missing language evaluations:\")\n",
    "    for item in missing_evals:\n",
    "        print(f\"  {item['model']}: {item['condition']} (expect {item['expected_lang']})\")\n",
    "        print(f\"    Response file: {item['response_file']}\")\n",
    "else:\n",
    "    print(\"All language evaluations are complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
