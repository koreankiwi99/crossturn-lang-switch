{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Lingual Transfer (X→Y) Analysis\n",
    "\n",
    "Analyzes language switching between non-English languages to examine whether English has a privileged role.\n",
    "\n",
    "Test pairs:\n",
    "- ZH→DE (Chinese context, German query)\n",
    "- DE→ZH (German context, Chinese query)\n",
    "- ES→AR (Spanish context, Arabic query)\n",
    "- AR→ES (Arabic context, Spanish query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport glob\nimport pandas as pd\nfrom pathlib import Path\n\n# Change to project root\n#os.chdir(Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent)\n#print(f\"Working directory: {os.getcwd()}\")\n\nMODELS = {\n    'gpt-5': 'GPT-5',\n    'claude-opus-4.5': 'Claude Opus 4.5',\n}\n\n# Cross-lingual pairs: (context_lang, query_lang) -> expected_response_lang\nCROSS_LINGUAL_PAIRS = [\n    ('zh', 'de'),  # ZH→DE: Chinese context, German query, expect German response\n    ('de', 'zh'),  # DE→ZH: German context, Chinese query, expect Chinese response\n    ('es', 'ar'),  # ES→AR: Spanish context, Arabic query, expect Arabic response\n    ('ar', 'es'),  # AR→ES: Arabic context, Spanish query, expect Spanish response\n]\n\nprint(f\"Models: {list(MODELS.keys())}\")\nprint(f\"Cross-lingual pairs: {CROSS_LINGUAL_PAIRS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Task Accuracy (Layer 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5 ZH→DE: 98/182 = 53.8%\n"
     ]
    }
   ],
   "source": [
    "def load_task_accuracy(model, from_lang, to_lang):\n",
    "    \"\"\"Load task accuracy from evaluated JSONL file.\"\"\"\n",
    "    condition = f\"{from_lang}_to_{to_lang}\"\n",
    "    \n",
    "    # Try cross-lingual directory first\n",
    "    patterns = [\n",
    "        f'../results/cross-lingual/{model}/evaluated_{condition}_*.jsonl',\n",
    "        f'../results/cross-lingual/{model}/evaluated_{condition}.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        if files:\n",
    "            stats = {'total': 0, 'passed': 0, 'failed': 0, 'error': 0}\n",
    "            with open(files[-1]) as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        stats['total'] += 1\n",
    "                        if data.get('evaluation', {}).get('passed'):\n",
    "                            stats['passed'] += 1\n",
    "                        else:\n",
    "                            stats['failed'] += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        stats['error'] += 1\n",
    "            return stats\n",
    "    return None\n",
    "\n",
    "# Test loading\n",
    "test = load_task_accuracy('gpt-5', 'zh', 'de')\n",
    "if test:\n",
    "    print(f\"GPT-5 ZH→DE: {test['passed']}/{test['total']} = {test['passed']/test['total']*100:.1f}%\")\n",
    "else:\n",
    "    print(\"No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Language Fidelity (Layer 1)\n",
    "\n",
    "For cross-lingual pairs, the expected response language is the query language (the target of the switch).\n",
    "- ZH→DE: expect German (de)\n",
    "- DE→ZH: expect Chinese (zh)\n",
    "- ES→AR: expect Arabic (ar)\n",
    "- AR→ES: expect Spanish (es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5 ZH→DE: 96.2%\n",
      "GPT-5 DE→ZH: 97.3%\n",
      "GPT-5 ES→AR: 96.2%\n",
      "GPT-5 AR→ES: 98.4%\n"
     ]
    }
   ],
   "source": [
    "def load_language_fidelity(model, from_lang, to_lang):\n",
    "    \"\"\"Load language fidelity from language_eval JSONL file.\n",
    "    \n",
    "    Expected response language is the query language (to_lang).\n",
    "    \"\"\"\n",
    "    condition = f\"{from_lang}_to_{to_lang}\"\n",
    "    expected_lang = to_lang  # Response should be in query language\n",
    "    \n",
    "    # Try to find language eval file with correct expected language\n",
    "    patterns = [\n",
    "        f'../results/cross-lingual/{model}/language_eval_{condition}_*.jsonl',\n",
    "        f'../results/cross-lingual/{model}/language_eval_{expected_lang}_*.jsonl',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        if files:\n",
    "            # Check if file has correct expected language\n",
    "            stats = {'total': 0, 'match': 0, 'mismatch': 0, 'error': 0}\n",
    "            correct_file = None\n",
    "            \n",
    "            for f_path in files:\n",
    "                with open(f_path) as f:\n",
    "                    first_line = f.readline()\n",
    "                    if first_line:\n",
    "                        data = json.loads(first_line)\n",
    "                        if data.get('expected_language') == expected_lang:\n",
    "                            correct_file = f_path\n",
    "                            break\n",
    "            \n",
    "            if correct_file:\n",
    "                with open(correct_file) as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            data = json.loads(line.strip())\n",
    "                            stats['total'] += 1\n",
    "                            status = data.get('match_status', '')\n",
    "                            if status == 'match':\n",
    "                                stats['match'] += 1\n",
    "                            elif status == 'mismatch':\n",
    "                                stats['mismatch'] += 1\n",
    "                            else:\n",
    "                                stats['error'] += 1\n",
    "                        except json.JSONDecodeError:\n",
    "                            stats['error'] += 1\n",
    "                return stats\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test\n",
    "for from_l, to_l in CROSS_LINGUAL_PAIRS:\n",
    "    stats = load_language_fidelity('gpt-5', from_l, to_l)\n",
    "    if stats:\n",
    "        fidelity = stats['match'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "        print(f\"GPT-5 {from_l.upper()}→{to_l.upper()}: {fidelity:.1f}%\")\n",
    "    else:\n",
    "        print(f\"GPT-5 {from_l.upper()}→{to_l.upper()}: No data (need to run language eval)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Results Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK ACCURACY (%)\n",
      "================================================================================\n",
      "            ZH→DE  DE→ZH  ES→AR  AR→ES\n",
      "Model                                 \n",
      "GPT-5        53.8   52.2   56.6   51.6\n",
      "Claude 4.5   47.3   46.2   51.1   48.9\n"
     ]
    }
   ],
   "source": [
    "# Build task accuracy table\n",
    "accuracy_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for from_l, to_l in CROSS_LINGUAL_PAIRS:\n",
    "        col_name = f\"{from_l.upper()}→{to_l.upper()}\"\n",
    "        stats = load_task_accuracy(model_id, from_l, to_l)\n",
    "        if stats and stats['total'] > 0:\n",
    "            row[col_name] = stats['passed'] / stats['total'] * 100\n",
    "        else:\n",
    "            row[col_name] = None\n",
    "    accuracy_data.append(row)\n",
    "\n",
    "df_accuracy = pd.DataFrame(accuracy_data).set_index('Model')\n",
    "print(\"TASK ACCURACY (%)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_accuracy.round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUAGE FIDELITY (%)\n",
      "================================================================================\n",
      "            ZH→DE  DE→ZH  ES→AR  AR→ES\n",
      "Model                                 \n",
      "GPT-5        96.2   97.3   96.2   98.4\n",
      "Claude 4.5   64.3   35.2   81.3   19.2\n"
     ]
    }
   ],
   "source": [
    "# Build language fidelity table\n",
    "fidelity_data = []\n",
    "for model_id, model_name in MODELS.items():\n",
    "    row = {'Model': model_name}\n",
    "    for from_l, to_l in CROSS_LINGUAL_PAIRS:\n",
    "        col_name = f\"{from_l.upper()}→{to_l.upper()}\"\n",
    "        stats = load_language_fidelity(model_id, from_l, to_l)\n",
    "        if stats and stats['total'] > 0:\n",
    "            row[col_name] = stats['match'] / stats['total'] * 100\n",
    "        else:\n",
    "            row[col_name] = None\n",
    "    fidelity_data.append(row)\n",
    "\n",
    "df_fidelity = pd.DataFrame(fidelity_data).set_index('Model')\n",
    "print(\"LANGUAGE FIDELITY (%)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_fidelity.round(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sanity Check: Errors and Empty Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_response_errors(model, from_lang, to_lang):\n    \"\"\"Check response files for API errors and empty responses.\"\"\"\n    condition = f\"{from_lang}_to_{to_lang}\"\n    pattern = f'../results/cross-lingual/{model}/responses_{condition}_*.jsonl'\n    files = sorted(glob.glob(pattern))\n\n    if not files:\n        return None\n\n    stats = {\n        'total': 0,\n        'success': 0,\n        'api_error': 0,\n        'empty_response': 0,\n        'parse_error': 0\n    }\n\n    with open(files[-1]) as f:\n        for line in f:\n            try:\n                data = json.loads(line.strip())\n                stats['total'] += 1\n                if data.get('success'):\n                    stats['success'] += 1\n                    if not data.get('response') or data.get('response', '').strip() == '':\n                        stats['empty_response'] += 1\n                else:\n                    stats['api_error'] += 1\n            except json.JSONDecodeError:\n                stats['parse_error'] += 1\n\n    return stats\n\n# Run sanity check\nprint(\"SANITY CHECK: Response Errors\")\nprint(\"=\" * 80)\n\nall_clean = True\nfor model_id, model_name in MODELS.items():\n    print(f\"\\n{model_name}:\")\n    has_errors = False\n    for from_l, to_l in CROSS_LINGUAL_PAIRS:\n        condition = f\"{from_l}_to_{to_l}\"\n        stats = check_response_errors(model_id, from_l, to_l)\n        if stats:\n            errors = []\n            if stats['api_error'] > 0:\n                errors.append(f\"API errors: {stats['api_error']}\")\n            if stats['empty_response'] > 0:\n                errors.append(f\"Empty: {stats['empty_response']}\")\n            if stats['parse_error'] > 0:\n                errors.append(f\"Parse errors: {stats['parse_error']}\")\n            if errors:\n                print(f\"  {condition}: {', '.join(errors)}\")\n                has_errors = True\n                all_clean = False\n            else:\n                print(f\"  {condition}: OK {stats['success']}/{stats['total']}\")\n        else:\n            print(f\"  {condition}: NO FILE\")\n            all_clean = False\n\nif all_clean:\n    print(\"\\nAll response files are clean (no errors)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}